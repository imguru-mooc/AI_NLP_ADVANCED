{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.7.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'lm_head.weight', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='<s>', eos_token='</s>', pad_token='<pad>')\n",
    "gpt_model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1, 1)\n",
      "[[9964]]\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.array([tokenizer.encode('이때')])\n",
    "print(type(input_ids))\n",
    "print(input_ids.shape)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이때 이게 다 뭐랄까, 네. 이거 지금 그~ 김정은이 인제 뭐~ 네. 이~ 뭔가 좀 어~ 어~ 뭔가 뭐~ 어~ 뭔가 이~ 어~ 뭔가 좀 네. 어~ 이게 이~ 뭐~ 이렇게 좀 이렇게 뭐~ <unk>니까?\n",
      "예. 뭔가 이제 이렇게 뭐~ 이~ 어~ 좀 이~ 뭔가 이런 어~ 뭔가 이~ 뭐~ \n"
     ]
    }
   ],
   "source": [
    "input_ids = np.array([tokenizer.encode('이때')])\n",
    "# print(type(input_ids))\n",
    "output = gpt_model.generate(input_ids, max_length=100, do_sample=True, top_k=20)\n",
    "# print(output[0].numpy().tolist())\n",
    "sentence = tokenizer.decode(output[0].numpy().tolist())\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이때도 한국이 중국(러시아)의 편에 서진 않을 것이라는 점도 강조했다.\n",
      "이런 가운데 일본 NHK는 이날 아베 총리가 중국과 대만을 잇달아 방문하고 있고, 중국이 일본을 향해 경제보복 공세를 강화했다는 보도에 대해 \"중국 외교부 장관과 왕이 중국 외교부장을 만나면 이런 말을 들을 수도 있을 것\"이라고 예상했다.\n",
      "일본과 미국을 중심으로 한 안보 동맹에서 중국이 부상하는 상황에서, 이번 사안에 대한 중국의 반응은 \"한국과 대만에 대한 경제 보복\"이라고 강하게 비난한 것이다.\n",
      "이날\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.array([tokenizer.encode('이때')])\n",
    "# print(type(input_ids))\n",
    "output = gpt_model.generate(input_ids, max_length=100, do_sample=True, top_p=0.95)\n",
    "sentence = tokenizer.decode(output[0].numpy().tolist())\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['그때에 김첨지는 대수롭지 않은듯이,', '만일 김첨지가 주기를 띠지 않았던들 한 발을 대문에 들여놓았을 제 그곳을 지배하는 무시무시한 정적(靜寂) ― 폭풍우가 지나간 뒤의 바다 같은 정적이 다리가 떨렸으리라.']\n",
      "284\n"
     ]
    }
   ],
   "source": [
    "DATA_IN_PATH = './data_in/KOR/'\n",
    "TRAIN_DATA_FILE = 'finetune_data.txt'\n",
    "\n",
    "sents = [s[:-1] for s in open(DATA_IN_PATH + TRAIN_DATA_FILE, encoding='utf-8').readlines()]\n",
    "print(sents[0:2])\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "  for sent in sents:\n",
    "    bos_token = [tokenizer.bos_token_id]\n",
    "    eos_token = [tokenizer.eos_token_id]\n",
    "    sent = tokenizer.encode(sent ) \n",
    "    yield bos_token + sent + eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(get_data, output_types=tf.int32)\n",
    "dataset = dataset.padded_batch(batch_size=8, padded_shapes=(None,), padding_values=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 52)\n",
      "tf.Tensor(\n",
      "[[    0 12858  8022  9324  8364  9272 20897 33336 10091 23272     1     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3]\n",
      " [    0 19089  9324  8364  9737 42414 10481  8263 18318  7285  9036 19846\n",
      "   9026 10924 15994  7128 23928  9037 35433 21299 15218  7556 27232  9040\n",
      "  11360  6433  2010   384   739   679 39798 15576 50866 16358 11099  9239\n",
      "   9040 10018 23948  9940  7422 17082 25462     1     3     3     3     3\n",
      "      3     3     3     3]\n",
      " [    0 30181  9337  9164 13530 41732  9080  9548  9784  8139  7513  9989\n",
      "   9079 28936   739  6976 22506 27533  7763  8463  7235 24257  8420 10171\n",
      "   7182 45006  9208  6889  8689  9796  9056  7791  9129  7395  9366  9818\n",
      "   7198  7321  9174 20579 19897 11520  9676  9220  8006  8528  8137  9036\n",
      "  15059  8367 18005     1]\n",
      " [    0  9676  9220  6949  8397  6949 25715  9022 24702 13647  9080 48270\n",
      "   9848  7285 18263  8277  8022  9037 15309  9026 18174     1     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3]\n",
      " [    0 10850  7354 28805 11968  9117  7445  8006  8210  6855  7489 40183\n",
      "   8146 10668  8811 18174     1     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3]\n",
      " [    0  9324  8364  9272  9099  7555 25715 29205 14184 23774  9239 14492\n",
      "   9094 18174     1     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3]\n",
      " [    0  9676 10452  7982 13541  9526  7731  8137 39146 14042  7162  9016\n",
      "      1     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3]\n",
      " [    0 10223  7965 12867  7235  9241  9023 12102 10137 17392 12627 20476\n",
      "  20191 12102 10652 18005     1     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3]], shape=(8, 52), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset:\n",
    "    print(batch.shape)\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "steps = len(sents) // 8 + 1\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc63e9b442a48efbc64c326dc93f37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 2.90381145\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0a8449bb6c4b3e9e3612ef947e1178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    2] cost = 1.60398388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ec794bdf2b4c0cba21b1d2e640efe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    3] cost = 1.00972819\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9a30e0a4284ee8beac9bb5b9becb51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    4] cost = 0.688100696\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06716b9e30df4cf9ac5dfdf524648b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    5] cost = 0.482445657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6f8e90565842479bdd68dda3d980ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    6] cost = 0.351334363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2873c34bb14e25875f4e9ae06d0b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    7] cost = 0.278075218\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b79e6ae79b4722b7bd18d369e5a3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    8] cost = 0.250238895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbf5e6b7adf4c68be39d94b63644841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    9] cost = 0.231242761\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a928691be98a46e782d69fb181158f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:   10] cost = 0.221696824\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for batch in tqdm.notebook.tqdm(dataset, total=steps):\n",
    "      with tf.GradientTape() as tape:\n",
    "          result = gpt_model(batch, labels=batch)\n",
    "          loss = result[0]\n",
    "          batch_loss = tf.reduce_mean(loss)\n",
    "          \n",
    "      grads = tape.gradient(batch_loss, gpt_model.trainable_variables)\n",
    "      adam.apply_gradients(zip(grads, gpt_model.trainable_variables))\n",
    "      epoch_loss += batch_loss / steps\n",
    "\n",
    "  print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_OUT_PATH = './data_out'\n",
    "model_name = \"tf2_gpt2_finetuned_model\"\n",
    "\n",
    "save_path = os.path.join(DATA_OUT_PATH, model_name)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "gpt_model.save_weights(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이때 “에미를 붙을 이 오라질 놈들 같으니, 이놈 내가 돈이 없을 줄 알고.”\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.array([tokenizer.encode('이때')])\n",
    "output = gpt_model.generate(input_ids, max_length=100)\n",
    "sentence = tokenizer.decode(output[0].numpy().tolist(),skip_special_tokens=True)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김첨지사는 구걸하는 거지나 무엇같이 연해연방 그의 기색을 살피며,\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.array([tokenizer.encode('김첨지')])\n",
    "output = gpt_model.generate(input_ids, max_length=100, do_sample=True, top_p=0.95)\n",
    "sentence = tokenizer.decode(output[0].numpy().tolist(),skip_special_tokens=True)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
