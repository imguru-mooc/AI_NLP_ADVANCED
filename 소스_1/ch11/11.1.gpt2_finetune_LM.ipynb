{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.0.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'lm_head.weight', 'transformer.h.7.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.11.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='<s>', eos_token='</s>', pad_token='<pad>')\n",
    "gpt_model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이때만 해도 그 정도 수준은 아니다.\n",
      "오히려, 그 수준이라는 건, 한국도 비슷한 수준이 아닌가 하는 생각이 들 정도.</d> 게임 내에선 상당히 많은 게임들 중에서, '스타트렉'이라는 이름의 몬스터를 생성할 수 있다.\n",
      "맵에 위치한 맵에서 스타트렉을 처치하면 몬스터, 즉 아이템을 얻을 수 있는데, 여기에 더해 '스타트렉'이라는 이름이 더해졌다.\n",
      "맵이 너무 좁은데다가 캐릭터들 간의 밸런스 조정이 필요해서, '\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.array([tokenizer.encode('이때')])\n",
    "# print(type(input_ids))\n",
    "output = gpt_model.generate(input_ids, max_length=100, do_sample=True, top_k=20)\n",
    "sentence = tokenizer.decode(output[0].numpy().tolist())\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이때 전 세계에서 몰려들어가면 한국에서도 일본이나 베트남과는 달리 '한국산' 반송되는 경우가 많아 반송되는 경우가 많다.\n",
      "또한 베트남에 한국산 반송 여부가 확인되는 사례가 거의 없기 때문에 한국산을 사는 것에 대해 부담감이 적지 않다.</d> AAB는 미국의 프로듀서이자 디자이너, 작곡가, 엔지니어가 함께 일하는 회사이다.\n",
      "1980년대 초에 이 회사의 CEO가 되었다.\n",
      "또한 AAB는 2017년에 사장이 되었다.\n",
      "AB\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.array([tokenizer.encode('이때')])\n",
    "# print(type(input_ids))\n",
    "output = gpt_model.generate(input_ids, max_length=100, do_sample=True, top_p=0.95)\n",
    "sentence = tokenizer.decode(output[0].numpy().tolist())\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['그때에 김첨지는 대수롭지 않은듯이,', '만일 김첨지가 주기를 띠지 않았던들 한 발을 대문에 들여놓았을 제 그곳을 지배하는 무시무시한 정적(靜寂) ― 폭풍우가 지나간 뒤의 바다 같은 정적이 다리가 떨렸으리라.']\n",
      "284\n"
     ]
    }
   ],
   "source": [
    "DATA_IN_PATH = './data_in/KOR/'\n",
    "TRAIN_DATA_FILE = 'finetune_data.txt'\n",
    "\n",
    "sents = [s[:-1] for s in open(DATA_IN_PATH + TRAIN_DATA_FILE, encoding='utf-8').readlines()]\n",
    "print(sents[0:2])\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "  for sent in sents:\n",
    "    bos_token = [tokenizer.bos_token_id]\n",
    "    eos_token = [tokenizer.eos_token_id]\n",
    "    sent = tokenizer.encode(sent ) \n",
    "    yield bos_token + sent + eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(get_data, output_types=tf.int32)\n",
    "dataset = dataset.padded_batch(batch_size=8, padded_shapes=(None,), padding_values=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[    0 12858  8022  9324  8364  9272 20897 33336 10091 23272     1     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3]\n",
      " [    0 19089  9324  8364  9737 42414 10481  8263 18318  7285  9036 19846\n",
      "   9026 10924 15994  7128 23928  9037 35433 21299 15218  7556 27232  9040\n",
      "  11360  6433  2010   384   739   679 39798 15576 50866 16358 11099  9239\n",
      "   9040 10018 23948  9940  7422 17082 25462     1     3     3     3     3\n",
      "      3     3     3     3]\n",
      " [    0 30181  9337  9164 13530 41732  9080  9548  9784  8139  7513  9989\n",
      "   9079 28936   739  6976 22506 27533  7763  8463  7235 24257  8420 10171\n",
      "   7182 45006  9208  6889  8689  9796  9056  7791  9129  7395  9366  9818\n",
      "   7198  7321  9174 20579 19897 11520  9676  9220  8006  8528  8137  9036\n",
      "  15059  8367 18005     1]\n",
      " [    0  9676  9220  6949  8397  6949 25715  9022 24702 13647  9080 48270\n",
      "   9848  7285 18263  8277  8022  9037 15309  9026 18174     1     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3]\n",
      " [    0 10850  7354 28805 11968  9117  7445  8006  8210  6855  7489 40183\n",
      "   8146 10668  8811 18174     1     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3]\n",
      " [    0  9324  8364  9272  9099  7555 25715 29205 14184 23774  9239 14492\n",
      "   9094 18174     1     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3]\n",
      " [    0  9676 10452  7982 13541  9526  7731  8137 39146 14042  7162  9016\n",
      "      1     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3]\n",
      " [    0 10223  7965 12867  7235  9241  9023 12102 10137 17392 12627 20476\n",
      "  20191 12102 10652 18005     1     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3     3     3     3     3     3     3     3     3\n",
      "      3     3     3     3]], shape=(8, 52), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "steps = len(sents) // 8 + 1\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6444c02fab4844838a6ed8ccbb942ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 2.90390086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1a452c092d473ab428d0ba3b4b3b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    2] cost = 1.60405207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86be2f88ab924dabaa71d399e236bf25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    3] cost = 1.00972116\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ce682f9991411aab755ca5740ecb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    4] cost = 0.688038826\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0312bc6e4a604fe2b3861931e9e273b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    5] cost = 0.482192934\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ad3ce2280045579ab11517cd7653c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    6] cost = 0.349638581\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c9b3807263404193532b0121711c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    7] cost = 0.278613627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77423a45959046e1aba6ebde94ea152c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    8] cost = 0.251193196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263818c6301b4d89ab0733d44cec2de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    9] cost = 0.232570022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f57a95b4402419a8b9151340fbe2281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:   10] cost = 0.222360566\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for batch in tqdm.notebook.tqdm(dataset, total=steps):\n",
    "      with tf.GradientTape() as tape:\n",
    "          result = gpt_model(batch, labels=batch)\n",
    "          loss = result[0]\n",
    "          batch_loss = tf.reduce_mean(loss)\n",
    "          \n",
    "      grads = tape.gradient(batch_loss, gpt_model.trainable_variables)\n",
    "      adam.apply_gradients(zip(grads, gpt_model.trainable_variables))\n",
    "      epoch_loss += batch_loss / steps\n",
    "\n",
    "  print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './data_out'\n",
    "model_name = \"tf2_gpt2_finetuned_model\"\n",
    "\n",
    "save_path = os.path.join(DATA_OUT_PATH, model_name)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "gpt_model.save_weights(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이때 “에미를 붙을 이 오라질 놈들 같으니, 이놈 내가 돈이 없을 줄 알고.”\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.array([tokenizer.encode('이때')])\n",
    "output = gpt_model.generate(input_ids, max_length=100)\n",
    "sentence = tokenizer.decode(output[0].numpy().tolist(),skip_special_tokens=True)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이때 이 말이 저도 모를 사이에 불쑥 김첨지의 입에서 떨어졌다.\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.array([tokenizer.encode('이때')])\n",
    "output = gpt_model.generate(input_ids, max_length=100, do_sample=True, top_p=0.95)\n",
    "sentence = tokenizer.decode(output[0].numpy().tolist(),skip_special_tokens=True)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
