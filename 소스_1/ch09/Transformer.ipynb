{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpu = tf.config.experimental.list_physical_devices('GPU') # 내 컴에 장착된 GPU를 list로 반환\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(gpu[0], True) # GPU Memory Growth를 Enable\n",
    "except RuntimeError as e:\n",
    "    print(e) # Error 발생하면 Error 내용 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(50)\n",
    "a = a.reshape(-1, 1)\n",
    "a.shape\n",
    "a = np.arange(50)[:, np.newaxis]\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position=50\n",
    "# a = tf.range(position, dtype=tf.float32)\n",
    "# print(a)\n",
    "# print(a.numpy())\n",
    "# print(a.shape)\n",
    "\n",
    "a = tf.range(position, dtype=tf.float32)[:, tf.newaxis]\n",
    "print(a.shape)\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 128\n",
    "i = tf.range(d_model, dtype=tf.float32)\n",
    "print(i.shape)\n",
    "i = i[tf.newaxis, :]\n",
    "print(i.shape)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.pow(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = [1,2,3,4]\n",
    "tf.pow(2,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[2, 2], \n",
    "                 [3, 3]])\n",
    "y = tf.constant([[8, 16], \n",
    "                 [2, 3]])\n",
    "tf.pow(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "d_model = 128\n",
    "i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :] # (1,128)\n",
    "i // 2\n",
    "2 * (i // 2)\n",
    "tf.pow(10000,(2 * (i // 2)))\n",
    "(2 * (i // 2)) / tf.cast(d_model, tf.float32)\n",
    "y = tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "i\n",
    "plt.plot(i[0],y[0])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "d_model=128\n",
    "angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "# print(angles.shape)\n",
    "# print(angles)\n",
    "\n",
    "x = i[0].numpy()\n",
    "# print(x)\n",
    "y = angles[0].numpy()\n",
    "# print(y)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(1,5)\n",
    "# print(a.shape)\n",
    "a = np.arange(1,5).reshape(4,1)\n",
    "# print(a.shape)\n",
    "b = 3\n",
    "c = a*b  # (4,1)*() => (4,1)(4,1)\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(1,5).reshape(4,1)\n",
    "b = np.array([3])\n",
    "c = a*b  # (4,1)*(1,) => (4,1)*(4,1) => (4,1)\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(1,13).reshape(4,3)\n",
    "b = np.array([[3,],\n",
    "              [3,],\n",
    "              [3,],\n",
    "              [3,]])\n",
    "c = a*b  # (4,3)*(4,1) => (4,3)*(4,3)\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(1,4).reshape(1,3)\n",
    "print(a)\n",
    "b = np.array([[0],\n",
    "              [1],\n",
    "              [2],\n",
    "              [3]])\n",
    "print(b)\n",
    "c = a*b  # (1,3)*(4,1) => (4,3)(4,3)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.math.sin(90.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.math.sin(90.*np.pi/180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.math.sin(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "#         print(\"PositionalEncoding.__init__()\", position, d_model)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "#         print(\"PositionalEncoding.get_angles()\")\n",
    "#         print(position.shape)  # (50,1)\n",
    "#         print(i.shape)         # (1,128)\n",
    "#         print(d_model)         # 128\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "#         print(angles.shape)  # (1,128)\n",
    "#         print(angles)\n",
    "        return position * angles    #   (50,1)*(1,128) => (50,128)*(50,128) => (50,128)\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "#         print(\"PositionalEncoding.positional_encoding()\", position, d_model)\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],  # (50,1)\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],          # (1,128)\n",
    "            d_model=d_model)\n",
    "\n",
    "#         print(angle_rads[:10,:10])\n",
    "#         print(angle_rads.shape)  # (50,128)\n",
    "        \n",
    "#         print(angle_rads[:10, 0:11:2])\n",
    "#         print(angle_rads[:, 0::2].shape)\n",
    "        \n",
    "        # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "#         print(sines[:10,:10])\n",
    "#         print(sines.shape)\n",
    "        \n",
    "        # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "#         print(cosines[:10,:10])\n",
    "#         print(cosines.shape)\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "#         print(pos_encoding.shape)\n",
    "#         print(pos_encoding[:10,:10])\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "#         print(pos_encoding.shape)  # (1, 50,128)\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):  # (1,40,128)\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :] # (1,40,128)+(1,40,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pos_encoding = PositionalEncoding(50, 128)\n",
    "print(sample_pos_encoding)\n",
    "print(sample_pos_encoding.pos_encoding.shape)     # (1,50,128)\n",
    "                                                  # (N,T,D)\n",
    "                                            # tf.shape(inputs)[1]\n",
    "print(sample_pos_encoding.pos_encoding[:, :10, :10])\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 128))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1\n",
    "T=50\n",
    "D=128\n",
    "a = np.arange(N*T*D).reshape(N,T,D)\n",
    "inputs = tf.constant(a, dtype = tf.float32)\n",
    "result = sample_pos_encoding(inputs)\n",
    "print(result.shape)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = tf.constant(np.arange(6).reshape(2,3),dtype=tf.float32)\n",
    "print(query)\n",
    "\n",
    "key = tf.constant(np.ones((2,3)),dtype=tf.float32)\n",
    "print(key)\n",
    "\n",
    "matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "print(matmul_qk.shape)\n",
    "print(\"matmul_qk=\", matmul_qk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = tf.constant(np.arange(12).reshape(2,2,3),dtype=tf.float32)\n",
    "print(query)\n",
    "\n",
    "key = tf.constant(np.ones((2,2,3)),dtype=tf.float32)\n",
    "print(key)\n",
    "\n",
    "matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "print(matmul_qk.shape)\n",
    "print(\"matmul_qk=\", matmul_qk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = tf.constant(np.arange(24).reshape(1,2,3,4),dtype=tf.float32)\n",
    "print(query)\n",
    "\n",
    "key = tf.constant(np.ones((1,2,3,4)),dtype=tf.float32)\n",
    "print(key)\n",
    "\n",
    "matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "print(matmul_qk.shape)\n",
    "print(\"matmul_qk=\", matmul_qk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = tf.constant(np.arange(48).reshape(2,2,3,4),dtype=tf.float32)\n",
    "print(query)\n",
    "\n",
    "key = tf.constant(np.ones((2,2,3,4)),dtype=tf.float32)\n",
    "print(key)\n",
    "\n",
    "matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "print(matmul_qk.shape)\n",
    "print(\"matmul_qk=\", matmul_qk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "    # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n",
    "\n",
    "\n",
    "    # Q와 K의 곱. 어텐션 스코어 행렬.\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True) # (64,4,40,32)(64,4,32,40) =>  (64,4,40,40)\n",
    "#     print(\"matmul_qk.shape =\", matmul_qk.shape)         \n",
    "#     print(\"matmul_qk =\", matmul_qk)\n",
    "    # 스케일링\n",
    "    # dk의 루트값으로 나눠준다.\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "#     print(\"depth=\",depth)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "#     print(\"logits=\",logits)\n",
    "\n",
    "    # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n",
    "    # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)  # 1 * (-1000000000)  # (1,4,4,4) += (1,1,1,4)\n",
    "        \n",
    "#     print(\"logits=\",logits)\n",
    "        \n",
    "      \n",
    "\n",
    "    # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n",
    "    # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "#     print(\"attention_weights=\",attention_weights)  \n",
    "#     print(\"attention_weights.shape =\",attention_weights.shape)  # (64,4,40,40)\n",
    "\n",
    "    \n",
    "#     print(\"value.shape=\", value.shape)\n",
    "#     print(\"value=\", value)\n",
    "    # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    output = tf.matmul(attention_weights, value) # (64,4,40,40)(64,4,40,32) => (64,4,40,32)\n",
    "#     print(\"output.shape =\", output.shape) \n",
    "#     print(\"output =\", output) \n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
    "print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\n",
    "print(temp_out) # 어텐션 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)\n",
    "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
    "print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\n",
    "print(temp_out) # 어텐션 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], \n",
    "                      [0, 10, 0], \n",
    "                      [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
    "print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\n",
    "print(temp_out) # 어텐션 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = tf.keras.layers.Dense(4)  # weight = (?,4)  , bias = (4,)\n",
    "# dir(dense1)\n",
    "print(dense1.weights)\n",
    "x = tf.constant([[1,2,3]])\n",
    "print(x.shape)\n",
    "out = dense1(x)   # (1,3)(3,4)+(4,)\n",
    "print(dense1.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAA():\n",
    "    def __init__(self):\n",
    "        print(\"AAA.__init__()\")\n",
    "\n",
    "class BBB(AAA):\n",
    "    def __init__(self):\n",
    "        super(BBB, self).__init__()\n",
    "        print(\"BBB.__init__()\")        \n",
    "\n",
    "BBB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(5)\n",
    "a = np.reshape(a, (-1,1) )\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        \n",
    "#         print(\"MultiHeadAttention.__init__()\")\n",
    "        self.num_heads = num_heads     # 4\n",
    "        self.d_model = d_model         # 128\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        # d_model을 num_heads로 나눈 값.\n",
    "        # 논문 기준 : 64\n",
    "        self.depth = d_model // self.num_heads  # 32\n",
    "#         print(\"self.depth=\", self.depth)\n",
    "\n",
    "        # WQ, WK, WV에 해당하는 밀집층 정의\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)  # (?,128)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "#         print(self.query_dense.weights)\n",
    "        # WO에 해당하는 밀집층 정의\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)  # (128,128)\n",
    "\n",
    "  # num_heads 개수만큼 q, k, v를 split하는 함수\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "#         print(\"split_heads()\")\n",
    "#         print(inputs.shape)    # (64, 40, 128) => (64, 40, 4, 32)\n",
    "        inputs = tf.reshape(                                                                                                                     # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))  \n",
    "                # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "#         print(inputs.shape)  # (64, 40, 4, 32)\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "                             # (64, 40, 4, 32)\n",
    "                             # (64, 4, 40, 32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "#         print(\"tf.shape(query) = \", tf.shape(query))\n",
    "#         print(\"batch_size=\", batch_size)\n",
    "\n",
    "#         print(query[0,0,:10])\n",
    "#         print(key[0,0,:10])        \n",
    "        \n",
    "        # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
    "        # q : (batch_size, query의 문장 길이, d_model)  =>  (64,40,128)\n",
    "        # k : (batch_size, key의 문장 길이, d_model)\n",
    "        # v : (batch_size, value의 문장 길이, d_model)\n",
    "        # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
    "        query = self.query_dense(query)    # (64,40,128)(128,128) => (64,40,128)\n",
    "        key = self.key_dense(key)          # (64,40,128)(128,128) => (64,40,128)\n",
    "        value = self.value_dense(value)    # (64,40,128)(128,128) => (64,40,128)\n",
    "        \n",
    "#         print(self.query_dense.weights)\n",
    "#         print(query.shape)\n",
    "#         print(key.shape)\n",
    "#         print(value.shape)\n",
    "        \n",
    "#         print(query[0,0,:10])\n",
    "#         print(key[0,0,:10])\n",
    "\n",
    "        # 2. 헤드 나누기\n",
    "        # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "        # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "#         print(query.shape)\n",
    "#         print(key.shape)\n",
    "#         print(value.shape)\n",
    "        \n",
    "\n",
    "        # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
    "        # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "#         print('scaled_attention.shape=',scaled_attention.shape)\n",
    "        # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "#         print('scaled_attention.shape=',scaled_attention.shape)\n",
    "        \n",
    "        # 4. 헤드 연결(concatenate)하기\n",
    "        # (batch_size, query의 문장 길이, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "#         print('concat_attention.shape=',concat_attention.shape)  # (64,40,4,32) => (64,40,128)\n",
    "\n",
    "        # 5. WO에 해당하는 밀집층 지나기\n",
    "        # (batch_size, query의 문장 길이, d_model)\n",
    "        outputs = self.dense(concat_attention)  # (64,40,128)(128,128) => (64,40,128)\n",
    "#         print('outputs.shape=',outputs.shape)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(np.random.randn(64,40,128))\n",
    "# inputs = { 'query':x, 'key':x, 'value':x, 'mask':None }\n",
    "# mha = MultiHeadAttention(128,4)\n",
    "# mha(inputs)\n",
    "\n",
    "outputs = MultiHeadAttention(128,4)({ 'query':x, 'key':x, 'value':x, 'mask':None })\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[1, 21, 777, 0, 0]])\n",
    "print(x.shape)\n",
    "mask = tf.cast(tf.math.equal(x, 0), tf.float32)    \n",
    "print(mask.shape)\n",
    "mask = mask[:, tf.newaxis, tf.newaxis, :] \n",
    "print(mask.shape)    \n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[1, 21, 777, 0, 0],\n",
    "                 [23, 34, 31, 12, 0]])\n",
    "print(x.shape)\n",
    "mask = tf.cast(tf.math.equal(x, 0), tf.float32)    \n",
    "print(mask.shape)\n",
    "mask = mask[:, tf.newaxis, tf.newaxis, :] \n",
    "print(mask.shape)    \n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):  # (1,5)\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32) # (2,5)\n",
    "    # (batch_size, 1, 1, key의 문장 길이)\n",
    "#     print(mask) #  [[0, 0, 0, 1, 1],\n",
    "                #    [1, 1, 0, 0, 0]]  (2,5)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]  # (2,1,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_padding_mask(tf.constant([[1, 21, 777, 0, 0]])))  # (1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_padding_mask(tf.constant([[1, 21, 777, 0, 0],\n",
    "                                       [0, 0, 777, 23, 25]])))  # (2,1,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.constant([1.,2.,3.,4.,-100000000])\n",
    "attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(np.random.randn(1,4,128))\n",
    "\n",
    "mask = create_padding_mask(tf.constant([[1, 23, 777, 0]]))  # (1,1,1,4)\n",
    "\n",
    "outputs = MultiHeadAttention(128,4)({ 'query':x, 'key':x, 'value':x, 'mask':mask })\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(1)\n",
    "layer = tf.keras.layers.Dropout(.2, input_shape=(2,))\n",
    "data = np.arange(1, 11).reshape(5, 2).astype(np.float32)\n",
    "print(data)\n",
    "\n",
    "outputs = layer(data, training=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.constant(np.arange(32).reshape(8, 4) * 10, dtype=tf.float32)\n",
    "print(data)\n",
    "\n",
    "m = np.mean(data.numpy(), axis=1)\n",
    "s = np.std(data.numpy(), axis=1)\n",
    "print(m, s)\n",
    "\n",
    "layer = tf.keras.layers.LayerNormalization(axis=1)\n",
    "\n",
    "output = layer(data)\n",
    "print(layer.beta)\n",
    "print(layer.gamma)\n",
    "print(output)\n",
    "\n",
    "m = np.mean(output.numpy(), axis=1)\n",
    "s = np.std(output.numpy(), axis=1)\n",
    "print(m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_i = np.array([1111,2220,300,40,50])\n",
    "k = len(x_i)\n",
    "mean_i = sum(x_i[j] for j in range(k)) / k\n",
    "print(mean_i)\n",
    "var_i = sum((x_i[j] - mean_i) ** 2 for j in range(k)) / k\n",
    "print(var_i)\n",
    "\n",
    "x_i_normalized = (x_i - mean_i) / np.sqrt(var_i + 0.0000001)\n",
    "print(x_i_normalized)\n",
    "print(np.mean(x_i_normalized))\n",
    "print(np.std(x_i_normalized))\n",
    "\n",
    "gamma=2\n",
    "beta=3\n",
    "\n",
    "output_i = x_i_normalized * gamma + beta\n",
    "print(output_i)\n",
    "print(np.mean(output_i))\n",
    "print(np.std(output_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")   # (None,None,128)\n",
    "\n",
    "    # 인코더는 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")  # (None,1,1,None)\n",
    "\n",
    "    # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "            'mask': padding_mask # 패딩 마스크 사용\n",
    "        })       \n",
    "    \n",
    "#     print('attention.shape=', attention.shape)  # (64,40,128)\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "#     print('attention.shape=', attention.shape)  # (64,40,128)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention)  # (64,40,128) + (64,40,128)\n",
    "    \n",
    "#     print('attention.shape=', attention.shape)\n",
    "\n",
    "    # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)  # (64,40,128)(128,512)=>(64,40,512)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)  # (64,40,512)(512,128)=>(64,40,128)\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs)  # (64,40,128) + (64,40,128)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "#     print(\"inputs.shape=\",inputs.shape)  # (None,None)\n",
    "    # 인코더는 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\") # (None,1,1,None)\n",
    "#     print(\"padding_mask.shape=\",padding_mask.shape)\n",
    "\n",
    "    # 포지셔널 인코딩 + 드롭아웃                                          # (N,T)\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)   # (9000,128) => (N,T,D)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))              # (64,40,128)\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)      # (64,40,128)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # 인코더를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "          dropout=dropout, name=\"encoder_layer_{}\".format(i),\n",
    "      )([outputs, padding_mask])\n",
    "#         print('outputs.shape=',outputs.shape)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더의 테스트\n",
    "\n",
    "# 인코더의 입력\n",
    "x = tf.constant(np.arange(64*40).reshape(64,40))  # (N,T)\n",
    "print(x.shape)\n",
    "\n",
    "# 인코더의 패딩 마스크\n",
    "enc_padding_mask = tf.keras.layers.Lambda(\n",
    "    create_padding_mask, output_shape=(1, 1, 40),\n",
    "    name='enc_padding_mask')(x)\n",
    "\n",
    "print(enc_padding_mask.shape)\n",
    "\n",
    "# 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n",
    "enc_outputs = encoder(vocab_size=9000, num_layers=2, dff=512,\n",
    "    d_model=128, num_heads=4, dropout=0.2,)(inputs=[x, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n",
    "\n",
    "print(enc_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[ 0,  1,  2, 3],\n",
    "                 [-1,  0,  1, 2],\n",
    "                 [-2, -1,  0, 1],\n",
    "                 [-3, -2, -1, 0]])\n",
    "\n",
    "\n",
    "print(tf.linalg.band_part(a, 0, 0))\n",
    "print(tf.linalg.band_part(a, 1, -1))\n",
    "print(tf.linalg.band_part(a, 2, 1))\n",
    "print(tf.linalg.band_part(a, 0, -1))\n",
    "print(tf.linalg.band_part(a, -1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=5\n",
    "a = tf.ones((seq_len, seq_len))\n",
    "print(a)\n",
    "print(tf.linalg.band_part(a, -1, 0))\n",
    "print(1-tf.linalg.band_part(a, -1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "#     print(tf.shape(x)) # (1,5)\n",
    "    seq_len = tf.shape(x)[1]\n",
    "#     print(seq_len)  # 5\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "#     return look_ahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_look_ahead_mask(tf.constant([[5, 4, 1, 0, 0]])))  # (N,T) => (1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "\n",
    "    # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "            'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "            'mask': look_ahead_mask # 룩어헤드 마스크\n",
    "        })\n",
    "\n",
    "    # 잔차 연결과 층 정규화\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "            'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n",
    "            'mask': padding_mask # 패딩 마스크\n",
    "        })\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "\n",
    "    # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 포지셔널 인코딩 + 드롭아웃\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # 디코더를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "            dropout=dropout, name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size, num_layers, dff,\n",
    "                  d_model, num_heads, dropout,\n",
    "                  name=\"transformer\"):\n",
    "\n",
    "    # 인코더의 입력\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 디코더의 입력\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 인코더의 패딩 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # 디코더의 룩어헤드 마스크(첫번째 서브층)\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask, output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # 디코더의 패딩 마스크(두번째 서브층)\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n",
    "    enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "        d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n",
    "\n",
    "    # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n",
    "    dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "        d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 다음 단어 예측을 위한 출력층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)   # (1,40,128)(128,8178) => (1,40,8178)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_transformer = transformer(\n",
    "    vocab_size = 9000,\n",
    "    num_layers = 4,\n",
    "    dff = 512,\n",
    "    d_model = 128,\n",
    "    num_heads = 4,\n",
    "    dropout = 0.3,\n",
    "    name=\"small_transformer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    small_transformer, to_file='small_transformer.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1769392"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [1, 2]\n",
    "y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()  # 0.051, 2.302\n",
    "scce(y_true, y_pred).numpy() # 1.176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5453146"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [1, 2]\n",
    "y_pred = [[-1., -2, 3.], [-2., -3, 1.]]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # 0.051, 2.302\n",
    "scce(y_true, y_pred).numpy() # 1.176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")\n",
    "# Text(0.5, 0, 'Train Step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorflow_datasets 처음 설치 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorflow_datasets 업그레이드 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --user --upgrade tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.2\n"
     ]
    }
   ],
   "source": [
    "print(tfds.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv\", filename=\"ChatBotData.csv\")\n",
    "train_data = pd.read_csv('ChatBotData.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "챗봇 샘플의 개수 : 11823\n"
     ]
    }
   ],
   "source": [
    "print('챗봇 샘플의 개수 :', len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q        0\n",
      "A        0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12시 땡 !]\n"
     ]
    }
   ],
   "source": [
    "sentence = re.sub(r\"([?.!,])\", r\" \\1 \", \"   12시 땡!    \")\n",
    "sentence = sentence.strip()\n",
    "print(\"[%s]\"%sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "for sentence in train_data['Q']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    # ex) 12시 땡! -> 12시 땡 !\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    questions.append(sentence)\n",
    "answers = []\n",
    "for sentence in train_data['A']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    # ex) 12시 땡! -> 12시 땡 !\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    answers.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡 !', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네']\n",
      "['하루가 또 가네요 .', '위로해 드립니다 .', '여행은 언제나 좋죠 .', '여행은 언제나 좋죠 .', '눈살이 찌푸려지죠 .']\n",
      "11823\n"
     ]
    }
   ],
   "source": [
    "print(questions[:5])\n",
    "print(answers[:5])\n",
    "print(len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8178\n"
     ]
    }
   ],
   "source": [
    "# 서브워드텍스트인코더를 사용하여 질문, 답변 데이터로부터 단어 집합(Vocabulary) 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰에 대한 정수 부여.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시작 토큰 번호 : [8178]\n",
      "종료 토큰 번호 : [8179]\n",
      "단어 집합의 크기 : 8180\n"
     ]
    }
   ],
   "source": [
    "print('시작 토큰 번호 :',START_TOKEN)\n",
    "print('종료 토큰 번호 :',END_TOKEN)\n",
    "print('단어 집합의 크기 :',VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서브워드텍스트인코더 토크나이저의 .encode()를 사용하여 텍스트 시퀀스를 정수 시퀀스로 변환.\n",
    "print(questions[20])\n",
    "print('임의의 질문 샘플을 정수 인코딩 : {}'.format(tokenizer.encode(questions[20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서브워드텍스트인코더 토크나이저의 .encode()와 .decode() 테스트해보기\n",
    "# 임의의 입력 문장을 sample_string에 저장\n",
    "sample_string = questions[20]\n",
    "\n",
    "# encode() : 텍스트 시퀀스 --> 정수 시퀀스\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# decode() : 정수 시퀀스 --> 텍스트 시퀀스\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in tokenized_string:\n",
    "    print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "sentence1 = questions[20]\n",
    "print(sentence1)\n",
    "print(tokenizer.encode(sentence1))\n",
    "sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "print(sentence1)\n",
    "tokenized_inputs = []\n",
    "tokenized_inputs.append(sentence1)\n",
    "print(tokenized_inputs)\n",
    "tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "print(tokenized_inputs.shape)\n",
    "print(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이를 40으로 정의\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "#     print(sentence1)\n",
    "    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "\n",
    "    tokenized_inputs.append(sentence1)\n",
    "    tokenized_outputs.append(sentence2)\n",
    "\n",
    "  # 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "  print(tokenized_inputs[0])\n",
    "\n",
    "  return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('질문 데이터의 크기(shape) :', questions.shape)\n",
    "print('답변 데이터의 크기(shape) :', answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questions[0])\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더의 실제값 시퀀스에서는 시작 토큰을 제거해야 한다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1] # 디코더의 입력. 마지막 패딩 토큰이 제거된다.\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다.\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "# 임의의 샘플에 대해서 [:, :-1]과 [:, 1:]이 어떤 의미를 가지는지 테스트해본다.\n",
    "print(answers[0]) # 기존 샘플\n",
    "print(answers[:1][:, :-1]) # 마지막 패딩 토큰 제거하면서 길이가 39가 된다.\n",
    "print(answers[:1][:, 1:]) # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다. 길이는 역시 39가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL = 256\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dff=DFF,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  # 레이블의 크기는 (batch_size, MAX_LENGTH - 1)\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  print(sentence)\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "  print(sentence)\n",
    "\n",
    "  print(START_TOKEN + tokenizer.encode(sentence) + END_TOKEN)\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "  print(sentence)\n",
    "\n",
    "  output = tf.expand_dims(START_TOKEN, 0)\n",
    "  print(output)\n",
    "\n",
    "  # 디코더의 예측 시작\n",
    "  for i in range(MAX_LENGTH):\n",
    "    predictions = model(inputs=[sentence, output], training=False)\n",
    "    print(predictions.shape)   #  (1,1,8180)\n",
    "    # 현재(마지막) 시점의 예측 단어를 받아온다.\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    print(predictions.shape)   #  (1,1,8180)\n",
    "    \n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    print(predicted_id)\n",
    "#     print(tokenizer.decode(predicted_id[0]))\n",
    "\n",
    "    # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
    "    # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  print(output.shape)\n",
    "  print(tf.squeeze(output, axis=0).shape)\n",
    "  return tf.squeeze(output, axis=0)\n",
    "\n",
    "def predict(sentence):\n",
    "  prediction = evaluate(sentence)\n",
    "\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  날씨가 좋넹.  \n",
      "날씨가 좋넹 .\n",
      "[8178, 1456, 2212, 8157, 8054, 8107, 1, 8179]\n",
      "tf.Tensor([[8178 1456 2212 8157 8054 8107    1 8179]], shape=(1, 8), dtype=int32)\n",
      "tf.Tensor([[8178]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "(1, 1, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[100]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "(1, 2, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[42]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "(1, 3, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[4821]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "(1, 4, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[7355]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 5], shape=(2,), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "(1, 5, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 6], shape=(2,), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "(1, 6, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[8179]], shape=(1, 1), dtype=int32)\n",
      "(1, 6)\n",
      "(6,)\n",
      "Input:   날씨가 좋넹.  \n",
      "Output: 저도 사람이 찾아오려나봐요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"  날씨가 좋넹.  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고민이 있어\n",
      "고민이 있어\n",
      "[8178, 990, 86, 8179]\n",
      "tf.Tensor([[8178  990   86 8179]], shape=(1, 4), dtype=int32)\n",
      "tf.Tensor([[8178]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "(1, 1, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[228]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "(1, 2, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[1073]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "(1, 3, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[79]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "(1, 4, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[29]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 5], shape=(2,), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "(1, 5, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[2136]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 6], shape=(2,), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "(1, 6, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[8158]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 7], shape=(2,), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "(1, 7, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[8082]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 8], shape=(2,), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "(1, 8, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[8107]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 9], shape=(2,), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "(1, 9, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[205]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([ 1 10], shape=(2,), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "(1, 10, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[6602]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([ 1 11], shape=(2,), dtype=int32)\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "(1, 11, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[2995]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([ 1 12], shape=(2,), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "(1, 12, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[308]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([ 1 13], shape=(2,), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n",
      "(1, 13, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[82]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([ 1 14], shape=(2,), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "(1, 14, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[71]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([ 1 15], shape=(2,), dtype=int32)\n",
      "tf.Tensor(15, shape=(), dtype=int32)\n",
      "(1, 15, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[25]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([ 1 16], shape=(2,), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "(1, 16, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([ 1 17], shape=(2,), dtype=int32)\n",
      "tf.Tensor(17, shape=(), dtype=int32)\n",
      "(1, 17, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[8179]], shape=(1, 1), dtype=int32)\n",
      "(1, 17)\n",
      "(17,)\n",
      "Input: 고민이 있어\n",
      "Output: 생각을 종이에 끄젹여여 보는게 도움이 될 수도 있어요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"고민이 있어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "너무 화가나\n",
      "너무 화가나\n",
      "[8178, 6, 3197, 8179]\n",
      "tf.Tensor([[8178    6 3197 8179]], shape=(1, 4), dtype=int32)\n",
      "tf.Tensor([[8178]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "(1, 1, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[5667]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "(1, 2, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[5467]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "(1, 3, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[7954]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "(1, 4, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[523]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 5], shape=(2,), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "(1, 5, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[314]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 6], shape=(2,), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "(1, 6, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[3]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 7], shape=(2,), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "(1, 7, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 8], shape=(2,), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "(1, 8, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[8179]], shape=(1, 1), dtype=int32)\n",
      "(1, 8)\n",
      "(8,)\n",
      "Input: 너무 화가나\n",
      "Output: 그럴수록 당신이 힘들 거예요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"너무 화가나\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카페갈래?\n",
      "카페갈래 ?\n",
      "[8178, 1984, 749, 384, 2, 8179]\n",
      "tf.Tensor([[8178 1984  749  384    2 8179]], shape=(1, 6), dtype=int32)\n",
      "tf.Tensor([[8178]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "(1, 1, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[1780]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "(1, 2, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[332]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "(1, 3, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[131]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "(1, 4, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 5], shape=(2,), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "(1, 5, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[8179]], shape=(1, 1), dtype=int32)\n",
      "(1, 5)\n",
      "(5,)\n",
      "Input: 카페갈래?\n",
      "Output: 카페 데이트 좋죠 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"카페갈래?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "게임하고싶당\n",
      "게임하고싶당\n",
      "[8178, 5717, 4600, 713, 8179]\n",
      "tf.Tensor([[8178 5717 4600  713 8179]], shape=(1, 5), dtype=int32)\n",
      "tf.Tensor([[8178]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "(1, 1, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[138]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "(1, 2, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[265]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "(1, 3, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[25]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "(1, 4, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 5], shape=(2,), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "(1, 5, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[8179]], shape=(1, 1), dtype=int32)\n",
      "(1, 5)\n",
      "(5,)\n",
      "Input: 게임하고싶당\n",
      "Output: 그럴 때가 있어요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"게임하고싶당\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "게임하자\n",
      "게임하자\n",
      "[8178, 5717, 148, 8179]\n",
      "tf.Tensor([[8178 5717  148 8179]], shape=(1, 4), dtype=int32)\n",
      "tf.Tensor([[8178]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "(1, 1, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[5717]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "(1, 2, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[68]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "(1, 3, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[41]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "(1, 4, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[8179]], shape=(1, 1), dtype=int32)\n",
      "(1, 4)\n",
      "(4,)\n",
      "Input: 게임하자\n",
      "Output: 게임하세요 !\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"게임하자\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결혼식이 너무 많아\n",
      "결혼식이 너무 많아\n",
      "[8178, 3734, 11, 6, 242, 8179]\n",
      "tf.Tensor([[8178 3734   11    6  242 8179]], shape=(1, 6), dtype=int32)\n",
      "tf.Tensor([[8178]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "(1, 1, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[6398]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "(1, 2, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[7595]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "(1, 3, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "(1, 4, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[8179]], shape=(1, 1), dtype=int32)\n",
      "(1, 4)\n",
      "(4,)\n",
      "Input: 결혼식이 너무 많아\n",
      "Output: 인맥이 넓으신가봐요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"결혼식이 너무 많아\")   # 결혼식이 너무 많아,인맥이 넓으신가봐요.,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결혼 두번 할까\n",
      "결혼 두번 할까\n",
      "[8178, 424, 5509, 98, 8179]\n",
      "tf.Tensor([[8178  424 5509   98 8179]], shape=(1, 5), dtype=int32)\n",
      "tf.Tensor([[8178]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "(1, 1, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[2975]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "(1, 2, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[936]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "(1, 3, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[14]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "(1, 4, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[7250]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 5], shape=(2,), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "(1, 5, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[4156]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 6], shape=(2,), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "(1, 6, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[97]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 7], shape=(2,), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "(1, 7, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([1 8], shape=(2,), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "(1, 8, 8180)\n",
      "(1, 1, 8180)\n",
      "tf.Tensor([[8179]], shape=(1, 1), dtype=int32)\n",
      "(1, 8)\n",
      "(8,)\n",
      "Input: 결혼 두번 할까\n",
      "Output: 사랑도 헤어짐도 면역이 안되나봐요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"결혼 두번 할까\") # 결혼할까"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
