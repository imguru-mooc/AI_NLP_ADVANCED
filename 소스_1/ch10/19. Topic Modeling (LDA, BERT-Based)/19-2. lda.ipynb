{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XS7PE94QLYu",
    "outputId": "590a773d-afca-40b0-b285-e574638193b5"
   },
   "outputs": [],
   "source": [
    "pip install pandas==1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5BFn70fAO5Hp",
    "outputId": "1cf8e660-0a3f-443d-b02e-fcf541eb8d54"
   },
   "outputs": [],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnVmgPuGOMjX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yYCxBOTEOV37",
    "outputId": "145a2052-970b-4e0a-80e2-25363eaaf1d5"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PvjipRJlOWoJ",
    "outputId": "7aec1f33-c020-4000-f64c-d94b0f359ad8"
   },
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "print('샘플의 수 :',len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "dxX3_-J1OXZS",
    "outputId": "1290dbee-ce9c-4a05-c3a2-19cdfd669ed2"
   },
   "outputs": [],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dKlU5yvsOYIK",
    "outputId": "78d482c9-348f-44db-88f5-5745dbabc868"
   },
   "outputs": [],
   "source": [
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bo8JCmnSOY2K",
    "outputId": "5b2b876c-64bc-4877-e86c-0c92fecc5678"
   },
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "_-mz9XwnOZm6",
    "outputId": "2385fb59-bfd1-47c5-9a2e-ece69c310055"
   },
   "outputs": [],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrzcPi8pOaUJ"
   },
   "outputs": [],
   "source": [
    "# NLTK로부터 불용어를 받아온다.\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBeWTOnVOhJ-",
    "outputId": "4e4d3ff5-14a6-4656-d9c1-1752cfc3769d"
   },
   "outputs": [],
   "source": [
    "tokenized_doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5W9Njj-fOmKV",
    "outputId": "0f285b56-7644-4faa-c92c-af427364eb29"
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tokenized_doc)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
    "\n",
    "# 수행된 결과에서 1번 인덱스 뉴스 출력\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gN0s9g4pOowa",
    "outputId": "73de13a2-6085-48b4-959f-d0dab88809b4"
   },
   "outputs": [],
   "source": [
    "print(dictionary[66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-sJOjEOUOpr5",
    "outputId": "46f7cc76-d38a-4aad-d150-c8776d2e1ee2"
   },
   "outputs": [],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cdzo2P2HOriR",
    "outputId": "f00cd7c4-2224-47a9-8e5c-e62982e7295d"
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS = 20\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "id": "to9fRQXQO6UO",
    "outputId": "eec33e51-cce0-4d0a-c80b-8ceaa349b78c"
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PT2ye60LO-uh",
    "outputId": "5c80e2c1-3de6-48c3-ac94-156703c46488"
   },
   "outputs": [],
   "source": [
    "for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "  if i==5:\n",
    "    break\n",
    "  print(i,'번째 문서의 topic 비율은',topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4HaCXJUPBgK"
   },
   "outputs": [],
   "source": [
    "def make_topictable_per_doc(ldamodel, corpus):\n",
    "    topic_table = pd.DataFrame()\n",
    "\n",
    "    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\n",
    "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
    "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
    "        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\n",
    "        # EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), \n",
    "        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\n",
    "        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.\n",
    "\n",
    "        # 모든 문서에 대해서 각각 아래를 수행\n",
    "        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\n",
    "            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\n",
    "                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n",
    "                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\n",
    "            else:\n",
    "                break\n",
    "    return(topic_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "4BLbadBuPD2y",
    "outputId": "a1711146-a6f3-4171-f9f1-18ea92d9bf7e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topictable = make_topictable_per_doc(ldamodel, corpus)\n",
    "topictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\n",
    "topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\n",
    "topictable[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 토픽 워드 클라우드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora \n",
    "from gensim import models\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "documents=[\n",
    "    '나는 아침에 라면을 자주 먹는다.',\n",
    "    '나는 아침에 밥 대신에 라면을 자주 먹는다.',\n",
    "    '현대인의 삶에서 스마트폰은 필수품이 되었다.',\n",
    "    '현대인들 중에서 스마트폰을 사용하지 않는 사람은 거의 없다. ',\n",
    "    '점심시간에 스마트폰을 이용해 영어 회화 공부를 하느라 혼자 밥을 먹는다.'\n",
    "]\n",
    "\n",
    "stoplist = ('.!?')                                        # 불용어 처리\n",
    "texts = [[word for word in document.split() if word not in stoplist]\n",
    "        for document in documents]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)                    # 사전 생성 (토큰화)\n",
    "print(dictionary)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]     # 말뭉치 생성 (벡터화)\n",
    "print('corpus : {}'.format(corpus))\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, \n",
    "        num_topics=2, random_state = 1)                   # 모델구축\n",
    "\n",
    "for t in lda.show_topics():  # 주제마다 출현 확률이 높은 단어 순으로 출력\n",
    "    print(t)\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "wc = WordCloud(background_color='white',\n",
    "        font_path='C:/windows/Fonts/malgunbd.ttf')            # 워드클라우드\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "for t in range(lda.num_topics):\n",
    "    plt.subplot(5,4,t+1)\n",
    "    x = dict(lda.show_topic(t,200))\n",
    "    im = wc.generate_from_frequencies(x)\n",
    "    plt.imshow(im)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Topic #\" + str(t))\n",
    "\n",
    "plt.savefig('LDA_wordcloud.png', bbox_inches='tight')     # 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LDA.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
