{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in c:\\users\\jikim\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\jikim\\anaconda3\\lib\\site-packages (from seqeval) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\jikim\\anaconda3\\lib\\site-packages (from seqeval) (1.20.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\jikim\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\jikim\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jikim\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed 고정\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3\n",
    "MAX_LEN = 111 # EDA에서 추출된 Max Length\n",
    "DATA_IN_PATH = 'data_in/KOR'\n",
    "DATA_OUT_PATH = \"data_out/KOR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개체명 인식 학습 데이터 개수: 81000\n",
      "개체명 인식 테스트 데이터 개수: 9000\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 준비\n",
    "DATA_TRAIN_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"train.tsv\")\n",
    "DATA_LABEL_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"label.txt\")\n",
    "DATA_TEST_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"test.tsv\")\n",
    "\n",
    "def read_file(input_path):\n",
    "    \"\"\"Read tsv file, and return words and label as list\"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            split_line = line.strip().split(\"\\t\")\n",
    "            sentences.append(split_line[0])\n",
    "            labels.append(split_line[1])\n",
    "        return sentences, labels\n",
    "\n",
    "train_sentences, train_labels = read_file(DATA_TRAIN_PATH)\n",
    "\n",
    "train_ner_dict = {\"sentence\": train_sentences, \"label\": train_labels}\n",
    "train_ner_df = pd.DataFrame(train_ner_dict)\n",
    "\n",
    "test_sentences, test_labels = read_file(DATA_TEST_PATH)\n",
    "test_ner_dict = {\"sentence\": test_sentences, \"label\": test_labels}\n",
    "test_ner_df = pd.DataFrame(test_ner_dict)\n",
    "\n",
    "print(\"개체명 인식 학습 데이터 개수: {}\".format(len(train_ner_df)))\n",
    "print(\"개체명 인식 테스트 데이터 개수: {}\".format(len(test_ner_df)))\n",
    "\n",
    "# 개체명 인식 학습 데이터 개수: 81000\n",
    "# 개체명 인식 테스트 데이터 개수: 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개체명 인식 레이블 개수: 30\n"
     ]
    }
   ],
   "source": [
    "# Label 불러오기\n",
    "\n",
    "def get_labels(label_path):\n",
    "    return [label.strip() for label in open(os.path.join(label_path), 'r', encoding='utf-8')]\n",
    "\n",
    "ner_labels = get_labels(DATA_LABEL_PATH)\n",
    "\n",
    "print(\"개체명 인식 레이블 개수: {}\".format(len(ner_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at bert_ckpt\\eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at bert_ckpt\\f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at bert_ckpt\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 버트 토크나이저 설정\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt')\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id # 0\n",
    "pad_token_label_id = 0\n",
    "cls_token_label_id = 0\n",
    "sep_token_label_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenizer(sent, MAX_LEN):\n",
    "    \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent,\n",
    "        truncation=True,\n",
    "        add_special_tokens = True, #'[CLS]'와 '[SEP]' 추가\n",
    "        max_length = MAX_LEN,           # 문장 패딩 및 자르기 진행\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True   # 어탠션 마스크 생성\n",
    "    )\n",
    "    \n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask'] \n",
    "    token_type_id = encoded_dict['token_type_ids']\n",
    "    \n",
    "    return input_id, attention_mask, token_type_id\n",
    "\n",
    "def convert_label(words, labels_idx, ner_begin_label, max_seq_len):\n",
    "            \n",
    "    tokens = []\n",
    "    label_ids = []\n",
    "\n",
    "    for word, slot_label in zip(words, labels_idx):\n",
    "\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        if not word_tokens:\n",
    "            word_tokens = [unk_token]\n",
    "        tokens.extend(word_tokens)\n",
    "        \n",
    "        # 슬롯 레이블 값이 Begin이면 I로 추가\n",
    "        if int(slot_label) in ner_begin_label:\n",
    "            label_ids.extend([int(slot_label)] + [int(slot_label) + 1] * (len(word_tokens) - 1))\n",
    "        else:\n",
    "            label_ids.extend([int(slot_label)] * len(word_tokens))\n",
    "  \n",
    "    # [CLS] and [SEP] 설정\n",
    "    special_tokens_count = 2\n",
    "    if len(label_ids) > max_seq_len - special_tokens_count:\n",
    "        label_ids = label_ids[: (max_seq_len - special_tokens_count)]\n",
    "\n",
    "    # [SEP] 토큰 추가\n",
    "    label_ids += [sep_token_label_id]\n",
    "\n",
    "    # [CLS] 토큰 추가\n",
    "    label_ids = [cls_token_label_id] + label_ids\n",
    "    \n",
    "    padding_length = max_seq_len - len(label_ids)\n",
    "    label_ids = label_ids + ([pad_token_label_id] * padding_length)\n",
    "    \n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28]\n",
      "['PER-B', 'FLD-B', 'AFW-B', 'ORG-B', 'LOC-B', 'CVL-B', 'DAT-B', 'TIM-B', 'NUM-B', 'EVT-B', 'ANM-B', 'PLT-B', 'MAT-B', 'TRM-B']\n"
     ]
    }
   ],
   "source": [
    "# 테스트용\n",
    "ner_begin_label = [ner_labels.index(begin_label) for begin_label in ner_labels if \"B\" in begin_label]\n",
    "ner_begin_label_string = [ner_labels[label_index] for label_index in ner_begin_label]\n",
    "\n",
    "print(ner_begin_label)\n",
    "print(ner_begin_label_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_begin_label = [ner_labels.index(begin_label) for begin_label in ner_labels if \"B\" in begin_label]\n",
    "\n",
    "def create_inputs_targets(df):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    label_list = []\n",
    "\n",
    "    for i, data in enumerate(df[['sentence', 'label']].values):\n",
    "        sentence, labels = data\n",
    "        words = sentence.split()\n",
    "        labels = labels.split()\n",
    "        labels_idx = []\n",
    "#         print(sentence)\n",
    "#         print(words)\n",
    "        \n",
    "        for label in labels:\n",
    "            labels_idx.append(ner_labels.index(label) if label in ner_labels else ner_labels.index(\"UNK\"))\n",
    "\n",
    "#         print(labels_idx)    \n",
    "        \n",
    "        assert len(words) == len(labels_idx)\n",
    "\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(sentence, MAX_LEN)\n",
    "\n",
    "        convert_label_id = convert_label(words, labels_idx, ner_begin_label, MAX_LEN)\n",
    "\n",
    "#         print(input_id)\n",
    "#         print(convert_label_id)\n",
    "#         break\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        label_list.append(convert_label_id)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    label_list = np.asarray(label_list, dtype=int) #레이블 토크나이징 리스트\n",
    "    inputs = (input_ids, attention_masks, token_type_ids)\n",
    "    \n",
    "    return inputs, label_list\n",
    "\n",
    "train_inputs, train_labels = create_inputs_targets(train_ner_df)\n",
    "test_inputs, test_labels = create_inputs_targets(test_ner_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertNERClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super(TFBertNERClassifier, self).__init__()\n",
    "\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(num_class, \n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range),\n",
    "                                                name=\"ner_classifier\")\n",
    "\n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "\n",
    "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs[0]\n",
    "                \n",
    "        sequence_output = self.dropout(sequence_output, training=training)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at bert_ckpt\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tf_model.h5 from cache at bert_ckpt\\879ba3c37de5c396bee982a9383f64cf08c9cc966d66742254a3904e6719357f.53d9b251a2a9d5d86139b64555b5e8deb0a20fc53f8a5ee958ddbb4506125a0b.h5\n",
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "ner_model = TFBertNERClassifier(model_name='bert-base-multilingual-cased',\n",
    "                                  dir_path='bert_ckpt',\n",
    "                                  num_class=len(ner_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(labels, logits):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "    )\n",
    "\n",
    "    # 0의 레이블 값은 손실 값을 계산할 때 제외\n",
    "    active_loss = tf.reshape(labels, (-1,)) != 0\n",
    "        \n",
    "    reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, logits.get_shape().as_list()[2])), active_loss)\n",
    "        \n",
    "    labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)\n",
    "    \n",
    "    return loss_fn(labels, reduced_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Metrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def compute_f1_pre_rec(self, labels, preds):\n",
    "\n",
    "        return {\n",
    "            \"precision\": precision_score(labels, preds, suffix=True),\n",
    "            \"recall\": recall_score(labels, preds, suffix=True),\n",
    "            \"f1\": f1_score(labels, preds, suffix=True)\n",
    "        }\n",
    "\n",
    "\n",
    "    def show_report(self, labels, preds):\n",
    "        return classification_report(labels, preds, suffix=True)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        results = {}\n",
    "        \n",
    "        pred = self.model.predict(self.x_eval)\n",
    "        label = self.y_eval\n",
    "        pred_argmax = np.argmax(pred, axis = 2)\n",
    "\n",
    "        slot_label_map = {i: label for i, label in enumerate(ner_labels)}\n",
    "\n",
    "        out_label_list = [[] for _ in range(label.shape[0])]\n",
    "        preds_list = [[] for _ in range(label.shape[0])]\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            for j in range(label.shape[1]):\n",
    "                if label[i, j] != 0:\n",
    "                    out_label_list[i].append(slot_label_map[label[i][j]])\n",
    "                    preds_list[i].append(slot_label_map[pred_argmax[i][j]])\n",
    "                    \n",
    "        result = self.compute_f1_pre_rec(out_label_list, preds_list)\n",
    "        results.update(result)\n",
    "\n",
    "        print(\"********\")\n",
    "        print(\"F1 Score\")\n",
    "        for key in sorted(results.keys()):\n",
    "            print(\"{}, {:.4f}\".format(key, results[key]))\n",
    "        print(\"\\n\" + self.show_report(out_label_list, preds_list))\n",
    "        print(\"********\")\n",
    "\n",
    "f1_score_callback = F1Metrics(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "# ner_model.compile(optimizer=optimizer, loss=compute_loss, run_eagerly=True)\n",
    "ner_model.compile(optimizer=optimizer, loss=compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_out/KOR\\tf2_bert_ner -- Folder already exists \n",
      "\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "2532/2532 [==============================] - ETA: 0s - loss: 0.4282WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "f1, 0.7430\n",
      "precision, 0.7071\n",
      "recall, 0.7827\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AFW       0.37      0.45      0.40       393\n",
      "         ANM       0.53      0.71      0.61       699\n",
      "         CVL       0.64      0.73      0.68      5735\n",
      "         DAT       0.80      0.90      0.85      2510\n",
      "         EVT       0.62      0.72      0.67      1093\n",
      "         FLD       0.40      0.40      0.40       228\n",
      "         LOC       0.66      0.78      0.72      2124\n",
      "         MAT       0.17      0.08      0.11        12\n",
      "         NUM       0.86      0.88      0.87      5544\n",
      "         ORG       0.72      0.77      0.74      4055\n",
      "         PER       0.76      0.82      0.79      4412\n",
      "         PLT       0.42      0.15      0.22        34\n",
      "         TIM       0.76      0.87      0.81       314\n",
      "         TRM       0.54      0.61      0.57      1950\n",
      "\n",
      "   micro avg       0.71      0.78      0.74     29103\n",
      "   macro avg       0.59      0.63      0.60     29103\n",
      "weighted avg       0.71      0.78      0.74     29103\n",
      "\n",
      "********\n",
      "2532/2532 [==============================] - 602s 234ms/step - loss: 0.4282\n",
      "Epoch 2/3\n",
      "2532/2532 [==============================] - ETA: 0s - loss: 0.2711WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "f1, 0.7784\n",
      "precision, 0.7697\n",
      "recall, 0.7873\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AFW       0.41      0.48      0.45       393\n",
      "         ANM       0.70      0.65      0.67       699\n",
      "         CVL       0.74      0.70      0.72      5735\n",
      "         DAT       0.89      0.89      0.89      2510\n",
      "         EVT       0.69      0.73      0.71      1093\n",
      "         FLD       0.51      0.56      0.53       228\n",
      "         LOC       0.77      0.76      0.77      2124\n",
      "         MAT       0.09      0.08      0.09        12\n",
      "         NUM       0.86      0.90      0.88      5544\n",
      "         ORG       0.77      0.80      0.78      4055\n",
      "         PER       0.80      0.84      0.82      4412\n",
      "         PLT       0.21      0.09      0.12        34\n",
      "         TIM       0.80      0.87      0.83       314\n",
      "         TRM       0.58      0.65      0.61      1950\n",
      "\n",
      "   micro avg       0.77      0.79      0.78     29103\n",
      "   macro avg       0.63      0.64      0.63     29103\n",
      "weighted avg       0.77      0.79      0.78     29103\n",
      "\n",
      "********\n",
      "2532/2532 [==============================] - 595s 235ms/step - loss: 0.2711\n",
      "Epoch 3/3\n",
      "2532/2532 [==============================] - ETA: 0s - loss: 0.2109WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "********\n",
      "F1 Score\n",
      "f1, 0.7900\n",
      "precision, 0.7636\n",
      "recall, 0.8183\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AFW       0.46      0.54      0.50       393\n",
      "         ANM       0.60      0.77      0.68       699\n",
      "         CVL       0.73      0.78      0.76      5735\n",
      "         DAT       0.89      0.91      0.90      2510\n",
      "         EVT       0.65      0.77      0.70      1093\n",
      "         FLD       0.54      0.54      0.54       228\n",
      "         LOC       0.74      0.81      0.78      2124\n",
      "         MAT       0.17      0.08      0.11        12\n",
      "         NUM       0.86      0.91      0.89      5544\n",
      "         ORG       0.78      0.81      0.79      4055\n",
      "         PER       0.81      0.83      0.82      4412\n",
      "         PLT       0.19      0.15      0.16        34\n",
      "         TIM       0.79      0.89      0.84       314\n",
      "         TRM       0.59      0.68      0.63      1950\n",
      "\n",
      "   micro avg       0.76      0.82      0.79     29103\n",
      "   macro avg       0.63      0.68      0.65     29103\n",
      "weighted avg       0.77      0.82      0.79     29103\n",
      "\n",
      "********\n",
      "2532/2532 [==============================] - 595s 235ms/step - loss: 0.2109\n",
      "{'loss': [0.42823031544685364, 0.2711232900619507, 0.21089564263820648]}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tf2_bert_ner\"\n",
    "\n",
    "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "history = ner_model.fit(train_inputs, train_labels, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                        callbacks=[cp_callback, f1_score_callback])\n",
    "\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model.save_weights('weights_NER.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model.load_weights('weights_NER.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   101   8982  16985  11287   9954   9477  15184  11261  20308  11513\n",
      "  23969   9672  11102 100699  10530  18154   9706 119285  12092  11506\n",
      "    119    102      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(test_inputs[0][0])\n",
    "print(test_inputs[1][0])\n",
    "print(test_inputs[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1,  1, 18, 19, 19, 19, 19, 19,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[ C L S ]', '손', '# # 흥', '# # 민', '# # 은', '토', '# # 트', '# # 넘', '경 기 에 서', '2', '# # 골', '# # 을', '넣', '# # 었 다', '.', '[ S E P ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]']\n",
      "['EVT-I', 'PER-B', 'PER-I', 'PER-I', 'PER-I', 'ORG-B', 'ORG-I', 'ORG-I', 'O', 'NUM-B', 'NUM-I', 'NUM-I', 'O', 'O', 'O', 'O', 'O', 'O', 'PER-B', 'O', 'O', 'O', 'ORG-I', 'ORG-I', 'O', 'O', 'NUM-I', 'NUM-B', 'NUM-I', 'NUM-I', 'O', 'O', 'O', 'O', 'O', 'PER-I', 'PER-I', 'PER-I', 'ORG-B', 'ORG-I', 'ORG-I', 'ORG-I', 'O', 'NUM-I', 'NUM-I', 'O', 'O', 'O', 'O', 'O', 'PER-I', 'PER-I', 'PER-I', 'ORG-B', 'O', 'PER-I', 'ORG-I', 'O', 'O', 'O', 'NUM-B', 'NUM-I', 'NUM-I', 'O', 'O', 'O', 'O', 'O', 'PER-I', 'PER-I', 'PER-I', 'ORG-B', 'ORG-I', 'ORG-I', 'ORG-I', 'NUM-B', 'NUM-I', 'NUM-I', 'O', 'O', 'O', 'O', 'O', 'PER-I', 'PER-I', 'PER-I', 'PER-I', 'PER-I', 'PER-I', 'ORG-B', 'ORG-I', 'ORG-I', 'ORG-I', 'NUM-B', 'NUM-I', 'NUM-I', 'O', 'O', 'O', 'O', 'O', 'PER-I', 'PER-I', 'PER-I', 'ORG-B', 'ORG-I', 'ORG-I', 'ORG-I', 'O', 'NUM-I', 'NUM-I']\n"
     ]
    }
   ],
   "source": [
    "sent = \"손흥민은 토트넘 경기에서 2골을 넣었다.\"\n",
    "\n",
    "# print(ner_labels)\n",
    "\n",
    "input_id, attention_mask, token_type_id = bert_tokenizer(sent, MAX_LEN)\n",
    "\n",
    "test_input_id = np.array(input_id, dtype=int).reshape(1,-1)\n",
    "test_attention_mask = np.array(attention_mask, dtype=int).reshape(1,-1)\n",
    "test_type_id = np.array(token_type_id, dtype=int).reshape(1,-1)\n",
    "test_input = (test_input_id, test_attention_mask, test_type_id)\n",
    "\n",
    "prediction = ner_model.predict(test_input)\n",
    "# print(prediction.shape)\n",
    "# print(prediction)\n",
    "idx = np.argmax(prediction[0], axis=1)\n",
    "temp_ner = [ ner_labels[i]  for i in idx ]\n",
    "\n",
    "token_print = [tokenizer.decode(token) for token in test_input_id[0]]\n",
    "print(token_print)\n",
    "print(temp_ner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
