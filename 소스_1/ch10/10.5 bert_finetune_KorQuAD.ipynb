{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U --user numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12607,
     "status": "ok",
     "timestamp": 1594010753269,
     "user": {
      "displayName": "ChangWook Jun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64",
      "userId": "00685987924881157185"
     },
     "user_tz": -540
    },
    "id": "B9WLyWEWgdDR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "import wget\n",
    "\n",
    "MAX_LEN = 512\n",
    "EPOCHS = 3\n",
    "VERBOSE = 2\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 556,
     "status": "ok",
     "timestamp": 1594010762115,
     "user": {
      "displayName": "ChangWook Jun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64",
      "userId": "00685987924881157185"
     },
     "user_tz": -540
    },
    "id": "68HVB3dYgi0w"
   },
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './data_out/KOR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 639,
     "status": "ok",
     "timestamp": 1594010763471,
     "user": {
      "displayName": "ChangWook Jun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64",
      "userId": "00685987924881157185"
     },
     "user_tz": -540
    },
    "id": "zvoswBdyglTQ"
   },
   "outputs": [],
   "source": [
    "def plot_graphs(history, string, string_1, string_2):\n",
    "    # loss \n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history[string_1])\n",
    "    plt.plot(history.history[string_2])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, string_1, string_2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_NUM = 1234\n",
    "tf.random.set_seed(SEED_NUM)\n",
    "np.random.seed(SEED_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65,
     "referenced_widgets": [
      "bc7f3c579a324f77811bdd6ad6dd7dc0",
      "e31de13423d743e68d6c451d23c93cdf",
      "f8f80478dfca4894ac1ff8c2a082f734",
      "3be3c9704e934fb5a3d5847749d398ce",
      "2c0ecef646d44a0580cacefa5c3fd9f2",
      "1fde406732df4b5b90b7701dc7e4981e",
      "f58154a65f974e04bcf8af24b2884fdd",
      "a7d4d0c48cda4abdb106a6bcfb24359e"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1217,
     "status": "ok",
     "timestamp": 1594010812799,
     "user": {
      "displayName": "ChangWook Jun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64",
      "userId": "00685987924881157185"
     },
     "user_tz": -540
    },
    "id": "HDI_cm3sgm6N",
    "outputId": "33078a97-0007-428b-9439-b67bd53cd994"
   },
   "outputs": [],
   "source": [
    "# Save the slow pretrained tokenizer\n",
    "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", lowercase=False)\n",
    "save_path = \"bert-base-multilingual-cased/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "slow_tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Load the fast tokenizer from saved file\n",
    "tokenizer = BertWordPieceTokenizer(\"bert-base-multilingual-cased/vocab.txt\", lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1750,
     "status": "ok",
     "timestamp": 1594010820826,
     "user": {
      "displayName": "ChangWook Jun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64",
      "userId": "00685987924881157185"
     },
     "user_tz": -540
    },
    "id": "an5cGi-GgpG4",
    "outputId": "c7753a24-f338-4a6d-8701-f78753f9b718"
   },
   "outputs": [],
   "source": [
    "train_data_url = \"https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\"\n",
    "train_path = keras.utils.get_file(\"train.json\", train_data_url)\n",
    "eval_data_url = \"https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\"\n",
    "eval_path = keras.utils.get_file(\"eval.json\", eval_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget.download('https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json', out='./bert-base-multilingual-cased/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename('./bert-base-multilingual-cased/bert-base-multilingual-cased-config.json', './bert-base-multilingual-cased/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget.download('https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-tf_model.h5', out='./bert-base-multilingual-cased/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename('./bert-base-multilingual-cased/bert-base-multilingual-cased-tf_model.h5', './bert-base-multilingual-cased/tf_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['밑바닥', '부터', '시작하는', '딥러닝']\n",
      "밑바닥 부터 시작하는 딥러닝\n"
     ]
    }
   ],
   "source": [
    "a = \"\"\"밑바닥 부터     시작하는 \n",
    "\n",
    "\n",
    "\n",
    "딥러닝\"\"\"\n",
    "print(str(a).split())\n",
    "a = \" \".join(str(a).split())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 99893,
     "status": "ok",
     "timestamp": 1594011009085,
     "user": {
      "displayName": "ChangWook Jun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64",
      "userId": "00685987924881157185"
     },
     "user_tz": -540
    },
    "id": "PkuK7N_ngrMd",
    "outputId": "48275df3-52de-4623-dfc3-db6be9a54dfa"
   },
   "outputs": [],
   "source": [
    "class SquadExample:\n",
    "    def __init__(self, question, context, start_char_idx, answer_text):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.start_char_idx = start_char_idx\n",
    "        self.answer_text = answer_text\n",
    "        self.skip = False\n",
    "\n",
    "    def preprocess(self):\n",
    "        context = self.context\n",
    "        question = self.question\n",
    "        answer_text = self.answer_text\n",
    "        start_char_idx = self.start_char_idx\n",
    "\n",
    "#         print('context=',context)\n",
    "#         print('question=',question)\n",
    "#         print('answer_text=',answer_text)\n",
    "#         print('start_char_idx=',start_char_idx)\n",
    "        # Clean context, answer and question\n",
    "        context = \" \".join(str(context).split())\n",
    "        question = \" \".join(str(question).split())\n",
    "        answer = \" \".join(str(answer_text).split())\n",
    "\n",
    "        # Find end character index of answer in context\n",
    "        end_char_idx = start_char_idx + len(answer)\n",
    "#         print('end_char_idx=',end_char_idx)\n",
    "        if end_char_idx >= len(context):\n",
    "            self.skip = True\n",
    "            return\n",
    "\n",
    "        # Mark the character indexes in context that are in answer\n",
    "        is_char_in_ans = [0] * len(context)\n",
    "        for idx in range(start_char_idx, end_char_idx):\n",
    "            is_char_in_ans[idx] = 1\n",
    "        \n",
    "#         print(\"is_char_in_ans = \", is_char_in_ans)\n",
    "\n",
    "        # Tokenize context\n",
    "        tokenized_context = tokenizer.encode(context)\n",
    "\n",
    "# [ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]        \n",
    "#         print(\"tokenized_context = \", tokenized_context)\n",
    "#         print('tokenized_context.ids=', tokenized_context.ids)\n",
    "#         print('tokenized_context.type_ids=', tokenized_context.type_ids)\n",
    "#         print('tokenized_context.tokens=', tokenized_context.tokens)\n",
    "#         print('tokenized_context.offsets=',tokenized_context.offsets)\n",
    "#         print('tokenized_context.attention_mask=', tokenized_context.attention_mask)\n",
    "#         print('tokenized_context.special_tokens_mask=', tokenized_context.special_tokens_mask)\n",
    "        \n",
    "        # Find tokens that were created from answer characters\n",
    "        ans_token_idx = []\n",
    "        for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
    "            if sum(is_char_in_ans[start:end]) > 0:\n",
    "                ans_token_idx.append(idx)\n",
    "                \n",
    "#         print(\"ans_token_idx=\", ans_token_idx)\n",
    "\n",
    "        if len(ans_token_idx) == 0:\n",
    "            self.skip = True\n",
    "            return\n",
    "\n",
    "        # Find start and end token index for tokens from answer\n",
    "        start_token_idx = ans_token_idx[0]\n",
    "        end_token_idx = ans_token_idx[-1]\n",
    "        \n",
    "#         print('start_token_idx=', start_token_idx)\n",
    "#         print('end_token_idx=', end_token_idx)\n",
    "\n",
    "        # Tokenize question\n",
    "        tokenized_question = tokenizer.encode(question)\n",
    "\n",
    "        # Create inputs\n",
    "\n",
    "#         print('tokenized_question.ids=', tokenized_question.ids)\n",
    "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
    "#         print('input_ids=', input_ids)\n",
    "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n",
    "            tokenized_question.ids[1:]\n",
    "        )\n",
    "#         print('token_type_ids=', token_type_ids)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "#         print('attention_mask=', attention_mask)\n",
    "\n",
    "#         print(\"len(input_ids)=\", len(input_ids))\n",
    "        # Pad and create attention masks.\n",
    "        # Skip if truncation is needed\n",
    "        padding_length = MAX_LEN - len(input_ids)\n",
    "        if padding_length > 0:  # pad\n",
    "            input_ids = input_ids + ([0] * padding_length)\n",
    "            attention_mask = attention_mask + ([0] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        elif padding_length < 0:  # skip\n",
    "            self.skip = True\n",
    "            return\n",
    "\n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.start_token_idx = start_token_idx\n",
    "        self.end_token_idx = end_token_idx\n",
    "        self.context_token_to_char = tokenized_context.offsets\n",
    "\n",
    "\n",
    "def create_squad_examples(raw_data):\n",
    "    squad_examples = []\n",
    "    for item in raw_data[\"data\"]:\n",
    "        for para in item[\"paragraphs\"]:\n",
    "            context = para[\"context\"]\n",
    "#             print('context = ' , context)\n",
    "            for qa in para[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "#                 print('question = ' , question)\n",
    "                \n",
    "                answer_text = qa[\"answers\"][0][\"text\"]\n",
    "#                 print('answer_text = ', answer_text)\n",
    "                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
    "#                 print('start_char_idx = ', start_char_idx)\n",
    "                squad_eg = SquadExample(\n",
    "                    question, context, start_char_idx, answer_text\n",
    "                )\n",
    "                squad_eg.preprocess()\n",
    "                squad_examples.append(squad_eg)\n",
    "#                 break\n",
    "#             break\n",
    "#         break\n",
    "    return squad_examples\n",
    "\n",
    "\n",
    "def create_inputs_targets(squad_examples):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"start_token_idx\": [],\n",
    "        \"end_token_idx\": [],\n",
    "    }\n",
    "    for item in squad_examples:\n",
    "        if item.skip == False:\n",
    "            for key in dataset_dict:\n",
    "                dataset_dict[key].append(getattr(item, key))\n",
    "#             print(dataset_dict)\n",
    "#             break\n",
    "            \n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "\n",
    "    x = [\n",
    "        dataset_dict[\"input_ids\"],\n",
    "        dataset_dict[\"token_type_ids\"],\n",
    "        dataset_dict[\"attention_mask\"],\n",
    "    ]\n",
    "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60407\n"
     ]
    }
   ],
   "source": [
    "with open(train_path) as f:\n",
    "    raw_train_data = json.load(f)\n",
    "\n",
    "with open(eval_path) as f:\n",
    "    raw_eval_data = json.load(f)\n",
    "\n",
    "#         self.input_ids = input_ids\n",
    "#         self.token_type_ids = token_type_ids\n",
    "#         self.attention_mask = attention_mask\n",
    "#         self.start_token_idx = start_token_idx\n",
    "#         self.end_token_idx = end_token_idx\n",
    "#         self.context_token_to_char = tokenized_context.offsets\n",
    "train_squad_examples = create_squad_examples(raw_train_data)\n",
    "print(len(train_squad_examples))\n",
    "# print(train_squad_examples[0].input_ids)\n",
    "# print(train_squad_examples[0].token_type_ids)\n",
    "# print(train_squad_examples[0].attention_mask)\n",
    "# print(train_squad_examples[0].start_token_idx)\n",
    "# print(train_squad_examples[0].end_token_idx)\n",
    "# print(train_squad_examples[0].context_token_to_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60407 training points created.\n",
      "5774 evaluation points created.\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
    "print(f\"{len(train_squad_examples)} training points created.\")\n",
    "\n",
    "eval_squad_examples = create_squad_examples(raw_eval_data)\n",
    "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
    "print(f\"{len(eval_squad_examples)} evaluation points created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대낮 시간대에는 덴노지 역 ~ 히네노 역 간에서 쾌속과 보통이 각각 시간당 6편씩 운전되고 있다. 히네노 역 이남 구간 ( ~ 와카야마 역 ) 의 운전 편수는 오사카 방면에서 온 대부분의 열차가 히네노 역에서 종착하기 때문에 히네노 역 이북 구간의 열차 편수에 비해 비교적 적은 편이다. 또 히네노 역 이남 지역의 특급의 우선도가 높기 때문에 히네노 역 · 이즈미스나가와 역에서 특급 열차와 보통 열차의 접속이 이루어지는 한편으로 쾌속 열차와의 접속 환경이 열악한 경우도 많기 때문에 쾌속 통과역과의 격차가 커지고 있다. 선내에서만 운전되는 열차 이외에 오사카 순환선, 간사이 공항선, 기세이 본선으로 직결 운행하는 열차도 설정되어 있다. 2008년 3월 15일 시간표 개정까지는 간사이 본선 JR 난바 역 발착의 열차도 존재했었다. 히네노 역 이남 구간과 이북 구간 중 열차 편수가 더 많은 곳은 어디인가?\n",
      "77 79\n",
      "132 137\n",
      "이북 구간\n"
     ]
    }
   ],
   "source": [
    "idx = 50\n",
    "\n",
    "# print(x_train[0][0])\n",
    "context = tokenizer.decode(x_train[0][idx])\n",
    "print(context)\n",
    "\n",
    "tokenized_context = tokenizer.encode(context)\n",
    "offset = tokenized_context.offsets\n",
    "\n",
    "start = y_train[0][idx]\n",
    "end = y_train[1][idx]\n",
    "print(start, end)\n",
    "\n",
    "start = offset[start][0]\n",
    "end = offset[end][1]\n",
    "\n",
    "print(start, end)\n",
    "print(context[start:end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1594011009787,
     "user": {
      "displayName": "ChangWook Jun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64",
      "userId": "00685987924881157185"
     },
     "user_tz": -540
    },
    "id": "mIjk3_XeguBj"
   },
   "outputs": [],
   "source": [
    "class TFBERTQuestionAnswering(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super(TFBERTQuestionAnswering, self).__init__()\n",
    "        \n",
    "        self.encoder = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
    "        self.start_logit = tf.keras.layers.Dense(num_class, name=\"start_logit\", use_bias=False) #(N,T,D)(D,1)=(N,T,1)\n",
    "        self.end_logit = tf.keras.layers.Dense(num_class, name=\"end_logit\", use_bias=False)\n",
    "        self.flatten = tf.keras.layers.Flatten() \n",
    "        self.softmax = tf.keras.layers.Activation(tf.keras.activations.softmax)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_ids, token_type_ids, attention_mask = inputs\n",
    "        embedding = self.encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "#         print(embedding.last_hidden_state)\n",
    "#         print(embedding[0])\n",
    "        embedding = embedding[0]\n",
    "        print(embedding.shape)   # (1,512,768)\n",
    "        start_logits = self.start_logit(embedding)\n",
    "        print(start_logits.shape) # (1,512,1)\n",
    "        start_logits = self.flatten(start_logits)\n",
    "        print(start_logits.shape) # (1,512)\n",
    "        \n",
    "        end_logits = self.end_logit(embedding)\n",
    "        end_logits = self.flatten(end_logits)\n",
    "        \n",
    "        start_probs = self.softmax(start_logits)\n",
    "        end_probs = self.softmax(end_logits)\n",
    "    \n",
    "        return start_probs, end_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korquad_model = TFBERTQuestionAnswering(model_name='./bert-base-multilingual-cased/',dir_path='bert_ckpt', num_class=1)\n",
    "inputs = [x_train[0][0:1] , x_train[1][0:1], x_train[2][0:1]]\n",
    "print(inputs[0].shape)\n",
    "start_probs, end_probs = korquad_model(inputs)\n",
    "print(np.argmax(start_probs.numpy))\n",
    "print(np.argmax(end_probs.numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11135,
     "status": "ok",
     "timestamp": 1594011020239,
     "user": {
      "displayName": "ChangWook Jun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64",
      "userId": "00685987924881157185"
     },
     "user_tz": -540
    },
    "id": "k4t_2T7vgwOu",
    "outputId": "fd7dcb5d-bf36-496c-b53d-53e89962360a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./bert-base-multilingual-cased/ were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ./bert-base-multilingual-cased/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "korquad_model = TFBERTQuestionAnswering(model_name='./bert-base-multilingual-cased/',dir_path='bert_ckpt', num_class=1)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1594011103474,
     "user": {
      "displayName": "ChangWook Jun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64",
      "userId": "00685987924881157185"
     },
     "user_tz": -540
    },
    "id": "YZtVFA3PgyL0"
   },
   "outputs": [],
   "source": [
    "def normalized_answer(s):    \n",
    "    def remove_(text):\n",
    "        ''' 불필요한 기호 제거 '''\n",
    "        text = re.sub(\"'\", \" \", text)\n",
    "        text = re.sub('\"', \" \", text)\n",
    "        text = re.sub('《', \" \", text)\n",
    "        text = re.sub('》', \" \", text)\n",
    "        text = re.sub('<', \" \", text)\n",
    "        text = re.sub('>', \" \", text) \n",
    "        text = re.sub('〈', \" \", text)\n",
    "        text = re.sub('〉', \" \", text)   \n",
    "        text = re.sub(\"\\(\", \" \", text)\n",
    "        text = re.sub(\"\\)\", \" \", text)\n",
    "        text = re.sub(\"‘\", \" \", text)\n",
    "        text = re.sub(\"’\", \" \", text)      \n",
    "        return text\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_punc(lower(remove_(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 720,
     "status": "ok",
     "timestamp": 1594011104061,
     "user": {
      "displayName": "ChangWook Jun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64",
      "userId": "00685987924881157185"
     },
     "user_tz": -540
    },
    "id": "rVTh1qKng1p8"
   },
   "outputs": [],
   "source": [
    "class ExactMatch(keras.callbacks.Callback):\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
    "        count = 0\n",
    "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
    "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
    "            squad_eg = eval_examples_no_skip[idx]\n",
    "            offsets = squad_eg.context_token_to_char\n",
    "            start = np.argmax(start)\n",
    "            end = np.argmax(end)\n",
    "            if start >= len(offsets):\n",
    "                continue\n",
    "            pred_char_start = offsets[start][0]\n",
    "            if end < len(offsets):\n",
    "                pred_char_end = offsets[end][1]\n",
    "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
    "            else:\n",
    "                pred_ans = squad_eg.context[pred_char_start:]\n",
    "\n",
    "            normalized_pred_ans = normalized_answer(pred_ans)\n",
    "            normalized_true_ans = normalized_answer(squad_eg.answer_text)\n",
    "            if normalized_pred_ans in normalized_true_ans:\n",
    "                count += 1\n",
    "        acc = count / len(self.y_eval[0])\n",
    "        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1594011104303,
     "user": {
      "displayName": "ChangWook Jun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64",
      "userId": "00685987924881157185"
     },
     "user_tz": -540
    },
    "id": "sTgvtk0og4Ow"
   },
   "outputs": [],
   "source": [
    "exact_match_callback = ExactMatch(x_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 599,
     "status": "ok",
     "timestamp": 1594011105561,
     "user": {
      "displayName": "ChangWook Jun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64",
      "userId": "00685987924881157185"
     },
     "user_tz": -540
    },
    "id": "7EuBYS58g6QZ"
   },
   "outputs": [],
   "source": [
    "korquad_model.compile(optimizer=optimizer, loss=[loss, loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 714,
     "status": "ok",
     "timestamp": 1594011106252,
     "user": {
      "displayName": "ChangWook Jun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64",
      "userId": "00685987924881157185"
     },
     "user_tz": -540
    },
    "id": "ZehxFPSrg8Q2",
    "outputId": "6a33f8a1-84d0-48c4-ac1e-5843daf1f2fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_out/KOR\\tf2_bert_korquad -- Folder already exists \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tf2_bert_korquad\"\n",
    "\n",
    "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18126376,
     "status": "ok",
     "timestamp": 1594029233934,
     "user": {
      "displayName": "ChangWook Jun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjljUh9LMLCM8kMgWLaX2xHiw2Cej8KoaOlkKxE=s64",
      "userId": "00685987924881157185"
     },
     "user_tz": -540
    },
    "id": "2ljuajCLmyws",
    "outputId": "e89526e8-e795-48df-eead-1a00b28005bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "(None, 512, 768)\n",
      "(None, 512, 1)\n",
      "(None, 512)\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_14/bert/pooler/dense/kernel:0', 'tf_bert_model_14/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "(None, 512, 768)\n",
      "(None, 512, 1)\n",
      "(None, 512)\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_14/bert/pooler/dense/kernel:0', 'tf_bert_model_14/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34532/1453719665.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history = korquad_model.fit(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# For demonstration, 3 epochs are recommended\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVERBOSE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1376\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1377\u001b[0m             with tf.profiler.experimental.Trace(\n\u001b[0;32m   1378\u001b[0m                 \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1244\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1246\u001b[1;33m       \u001b[0moriginal_spe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1247\u001b[0m       can_run_full_execution = (\n\u001b[0;32m   1248\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    676\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1221\u001b[0m     \"\"\"\n\u001b[0;32m   1222\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1223\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1224\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1187\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = korquad_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=EPOCHS,  # For demonstration, 3 epochs are recommended\n",
    "    verbose=VERBOSE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[exact_match_callback, cp_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxaigHy2m4JB"
   },
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss', 'output_1_loss', 'output_2_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korquad_model.save_weights('data_out/KOR/tf2_bert_korquad/weights_KorQuAD.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korquad_model.load_weights('data_out/KOR/tf2_bert_korquad/weights_KorQuAD.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korquad_model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(korquad_model.history,'loss', 'output_1_loss', 'output_2_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 predict 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "korquad_model.load_weights('data_out/KOR/tf2_bert_korquad/weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지문 찾아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" 내각과 장관들이 소외되고 대통령비서실의 권한이 너무 크다 \", \" 행보가 비서 본연의 역할을 벗어난다 \" 는 의견이 제기되었다. 대표적인 예가 10차 개헌안 발표이다. 원로 헌법학자인 허영 경희대 석좌교수는 정부의 헌법개정안 준비 과정에 대해 \" 청와대 비서실이 아닌 국무회의 중심으로 이뤄졌어야 했다 \" 고 지적했다.'국무회의의 심의를 거쳐야 한다'( 제89조 ) 는 헌법 규정에 충실하지 않았다는 것이다. 그러면서 \" 법무부 장관을 제쳐놓고 민정수석이 개정안을 설명하는 게 이해가 안 된다 \" 고 지적했다. 민정수석은 국회의원에 대해 책임지는 법무부 장관도 아니고, 국민에 대해 책임지는 사람도 아니기 때문에 정당성이 없고, 단지 대통령의 신임이 있을 뿐이라는 것이다. 또한 국무총리 선출 방식에 대한 기자의 질문에 \" 문 대통령도 취임 전에 국무총리에게 실질적 권한을 주겠다고 했지만 그러지 못하고 있다. 대통령비서실장만도 못한 권한을 행사하고 있다. \" 고 답변했다. 법무부 장관을 제쳐놓고 민정수석이 개정안을 설명하는 게 이해가 안 된다고 지적한 경희대 석좌교수 이름은?\n"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "print(tokenizer.decode(x_eval[0][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   101    107   8996  66540  11882   9657  20595  20173   9448  78705\n",
      "  29208  70672  29455  12424  31503  10459   8917  11102  10739   9004\n",
      "  32537   9834  11903    107    117    107   9966  30005  11287   9379\n",
      "  12424   9358  25486  10459  69144   9342  54305  11903    107   9043\n",
      "   9637 118634  10739   9672  12310  13628    119   9069  37824  15387\n",
      "   9576  11287  10150  23466   8857 119426  34951   9323  37824  11925\n",
      "    119   9612  11261   9969  33768 109522  12030   9968  30858   8885\n",
      "  49515  14423   9426 119215  25242  93835   9670  43875   9969  33768\n",
      "  21789  16605  34951   9691  29455   8898  77763  33378    107   9751\n",
      "  12638  14423   9379  12424  31503  10739  63783   8909  32537  56356\n",
      "  75109   9638 118895 119210  12965  21711  23622    107   8888   9706\n",
      "  14801  12490    119    112   8909  32537  56356  10459   9491  78505\n",
      "  69642  21711  16139    112    113   9672  11396  11373  20626    114\n",
      "   9043   9969  33768   8922  77763   9770  31503  23665  49137  11018\n",
      "  24190    119   8924  30873  30936    107   9341  32537  14646   9657\n",
      "  20595  10622   9672 119265 118748  11664   9311  16605  15891  40958\n",
      "  10739   8857  16605  34951  10622   9429  16758  12178   8872   9638\n",
      "  14523  11287   9521  22143    107   8888   9706  14801  12490    119\n",
      "   9311  16605  15891  40958  10892   8909  56356 108280  33378   9739\n",
      "  36240  32815   9341  32537  14646   9657  20595  12092   9519  25503\n",
      "  11664    117   8909  36553  10530  33378   9739  36240  32815   9405\n",
      "  61250  12092   9519  25503  12310  20729   9670  21928  53371   9555\n",
      "  11664    117   9059  12508  70672  10459   9487  36240  10739  68943\n",
      "   9400  10739  60362  24190    119  19789   8909  32537 119270  12692\n",
      "   9428  52363   9328  21155  10530  18154  60886  10459   9709  25934\n",
      "  10530    107   9297  70672  12092  71963  91069   8909  32537 119270\n",
      "  46766  14153   9489  48599  14801   8917  11102  10622   9689 118632\n",
      "  85634   9965  28578   8924  30873  12508   9290  12453  11506    119\n",
      "  70672  29455  12424  31503  13890  19105  12092   9290  11102   8917\n",
      "  11102  10622   9966  12945  12453  11506    119    107   8888   9065\n",
      " 118985  12490    119    102   9341  32537  14646   9657  20595  10622\n",
      "   9672 119265 118748  11664   9311  16605  15891  40958  10739   8857\n",
      "  16605  34951  10622   9429  16758  12178   8872   9638  14523  11287\n",
      "   9521  22143  11664   9706  14801  11102   8885  49515  14423   9426\n",
      " 119215  25242  15891  78199    136    102      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(x_eval[0][idx])\n",
    "print(x_eval[1][idx])\n",
    "print(x_eval[2][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_context= tokenizer.encode(\"법무부 장관을 제쳐놓고 민정수석이 개정안을 설명하는 게 이해가 안 된다고 지적한 경희대 석좌교수 이름은?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "[(0, 0), (0, 1), (1, 2), (2, 3), (4, 5), (5, 6), (6, 7), (8, 9), (9, 10), (10, 11), (11, 12), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (19, 20), (20, 21), (21, 22), (22, 23), (24, 25), (25, 26), (26, 28), (29, 30), (31, 32), (32, 33), (33, 34), (35, 36), (37, 39), (39, 40), (41, 42), (42, 43), (43, 44), (45, 46), (46, 47), (47, 48), (49, 50), (50, 51), (51, 52), (52, 53), (54, 57), (57, 58), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "context_token_to_char = tokenized_context.offsets\n",
    "print(len(context_token_to_char))\n",
    "print(context_token_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 법 ##무 ##부 장 ##관 ##을 제 ##쳐 ##놓 ##고 민 ##정 ##수 ##석 ##이 개 ##정 ##안 ##을 설 ##명 ##하는 게 이 ##해 ##가 안 된다 ##고 지 ##적 ##한 경 ##희 ##대 석 ##좌 ##교 ##수 이름은 ?  "
     ]
    }
   ],
   "source": [
    "for i in range(len(context_token_to_char)):\n",
    "    print(tokenizer.decode([tokenized_context.ids[i]]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding_length= 310\n",
      "(None, 512, 768)\n",
      "(None, 512, 1)\n",
      "(None, 512)\n",
      "88 92\n",
      "(132, 133) (136, 138)\n",
      "런던의정서는\n"
     ]
    }
   ],
   "source": [
    "class SquadExample_pred:\n",
    "    def __init__(self, question, context):#, start_char_idx, answer_text):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.skip = False\n",
    "\n",
    "    def preprocess(self):\n",
    "        context = self.context\n",
    "        question = self.question\n",
    "        #answer_text = self.answer_text\n",
    "        #start_char_idx = self.start_char_idx\n",
    "\n",
    "#         print('context=',context)\n",
    "#         print('question=',question)\n",
    "        \n",
    "        # Clean context, answer and question\n",
    "        context = \" \".join(str(context).split())\n",
    "        question = \" \".join(str(question).split())\n",
    "        \n",
    "#         print('context=',context)\n",
    "#         print('question=',question)\n",
    "\n",
    "        # Tokenize context\n",
    "        tokenized_context = tokenizer.encode(context)\n",
    "#         print(\"tokenized_context.ids=\", tokenized_context.ids)\n",
    "#         print(\"tokenized_context.ids.len=\", len(tokenized_context.ids))\n",
    "\n",
    "        # Tokenize question\n",
    "        tokenized_question = tokenizer.encode(question)\n",
    "#         print(\"tokenized_question.ids=\", tokenized_question.ids)\n",
    "\n",
    "        # Create inputs\n",
    "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
    "#         print(\"input_ids=\", input_ids)\n",
    "        \n",
    "        \n",
    "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n",
    "            tokenized_question.ids[1:]\n",
    "        )\n",
    "#         print(\"token_type_ids=\", token_type_ids)\n",
    "        \n",
    "        attention_mask = [1] * len(input_ids)\n",
    "#         print(\"attention_mask=\", attention_mask)\n",
    "        \n",
    "\n",
    "        # Pad and create attention masks.\n",
    "        # Skip if truncation is needed\n",
    "        \n",
    "        padding_length = MAX_LEN - len(input_ids)\n",
    "        print(\"padding_length=\",padding_length)\n",
    "        \n",
    "        if padding_length > 0:  # pad\n",
    "            input_ids = input_ids + ([0] * padding_length)\n",
    "            attention_mask = attention_mask + ([0] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        elif padding_length < 0:  # skip\n",
    "            self.skip = True\n",
    "            return\n",
    "        \n",
    "#         print(\"input_ids=\", input_ids)\n",
    "#         print(\"token_type_ids=\", token_type_ids)\n",
    "#         print(\"attention_mask=\", attention_mask)\n",
    "\n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.context_token_to_char = tokenized_context.offsets\n",
    "#         print(self.context_token_to_char)\n",
    "        \n",
    "    def get_input_target(self):\n",
    "        dataset_dict = {\n",
    "            \"input_ids\": [],\n",
    "            \"token_type_ids\": [],\n",
    "            \"attention_mask\": [],\n",
    "        }\n",
    "        if self.skip == False:\n",
    "            for key in dataset_dict:\n",
    "                dataset_dict[key].append(getattr(self, key))\n",
    "        for key in dataset_dict:\n",
    "            dataset_dict[key] = np.array(dataset_dict[key])\n",
    "\n",
    "        x = [\n",
    "            dataset_dict[\"input_ids\"],\n",
    "            dataset_dict[\"token_type_ids\"],\n",
    "            dataset_dict[\"attention_mask\"],\n",
    "        ]\n",
    "#         print(x[0].shape)\n",
    "        return x\n",
    "\n",
    "def create_squad_examples_from_arg(question, context):#, start_char_idx, answer_text):\n",
    "    squad_eg = SquadExample_pred(\n",
    "        question, context#, start_char_idx, answer_text\n",
    "    )\n",
    "    squad_eg.preprocess()\n",
    "    return squad_eg\n",
    "\n",
    "\n",
    "def predict_test(model, pred_raw):\n",
    "    x_pred = pred_raw.get_input_target()\n",
    "    pred_start, pred_end = model.predict(x_pred)\n",
    "#     print(pred_start.shape)\n",
    "#     print(pred_end.shape)\n",
    "#     print(pred_start)\n",
    "    pred_start_offset_index = np.argmax(pred_start)\n",
    "    pred_end_offset_index = np.argmax(pred_end)\n",
    "    print(pred_start_offset_index, pred_end_offset_index)\n",
    "#     print(pred_raw.context_token_to_char)\n",
    "    pred_start_offset = pred_raw.context_token_to_char[pred_start_offset_index]\n",
    "    pred_end_offset = pred_raw.context_token_to_char[pred_end_offset_index]\n",
    "    print(pred_start_offset, pred_end_offset)\n",
    "\n",
    "    answer = pred_context[pred_start_offset[0]:pred_end_offset[1]]\n",
    "    \n",
    "    normalized_pred_ans = normalized_answer(answer)\n",
    "    \n",
    "    return normalized_pred_ans\n",
    "\n",
    "\n",
    "pred_context = '''정부가 이산화탄소 스트림의 수출을 가능하게 하는 런던의정서개정(2009년 개정) 수락서를 국제해사기구(IMO) 사무국에 기탁할 예정이라고 29일 밝혔다. 이산화탄소 스트림은 제철소나 발전소 등에서 포집한 이산화탄소다.\n",
    "1996년 채택된 런던의정서는 자국 해역에서 이산화탄소 스트림 격리(저장)는 허용하는 반면, 이산화탄소 스트림 수출(국가 간 이동)은 금지했다. \n",
    "2009년 당사국총회에서 일정 절차에 따라 이산화탄소 스트림 수출을 허용하는 개정안이 채택됐다. '''\n",
    "pred_question = \"이산화탄소 스트림 수출(국가 간 이동)은 금지하는 협정서는?\"\n",
    "\n",
    "pred_data = create_squad_examples_from_arg(pred_question, pred_context)\n",
    "pred_answer = predict_test(korquad_model, pred_data)\n",
    "print(pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "노아는 하나님의 명령에 따라 배를 만들고 가족과 정결한 짐승 암수 일곱 마리씩, 부정한 짐승 암수 한 마리씩(혹은 두 마리씩; 사본에 따라 다름), 그리고 새 암수 일곱 마리씩을 싣고 밀어닥친 홍수를 피하였다. 모든 사람들이 타락한 생활에 빠져 있어 하나님이 홍수로 심판하려 할 때 홀로 바르게 살던 노아는 하나님의 특별한 계시로 홍수가 올 것을 미리 알게 된다. 그는 길이 300 규빗, 너비 50 규빗, 높이 30 규빗(고대의 1규빗은 팔꿈치에서 가운데 손가락끝까지의 길이로 약 45~46cm를 가리킴), 상 ·중 ·하 3층으로 된 방주를 만들어 8명의 가족과, 한 쌍씩의 여러 동물을 데리고 이 방주에 탄다. 대홍수를 만나 모든 생물(물고기 제외)이 전멸하고 말았지만, 이 방주에 탔던 노아의 가족과 동물들은 살아 남았다고 한다.〈창세기〉 6장 14~16절에 보면 길이 300규빗 (약 135m), 폭 50 규빗 (약 22.5m), 높이 30 규빗 (약 13.5m)인 이 배는 지붕과 문을 달고 배 안은 3층으로 만들어져 있었다. 선체(船體)는 고페르나무(잣나무)로 되고 안쪽에는 역청(아스팔트와 비슷한 성분)을 칠하여 굳혔다고 기록하고 있다.\n",
      "하나님의 명령에 배를 만들고 가족과 짐승들을 배에 태워 홍수를 피한 사람은 누구인가?\n",
      "노아\n",
      "0 2\n",
      "padding_length= 97\n",
      "1 2\n",
      "(0, 1) (1, 2)\n",
      "노아\n"
     ]
    }
   ],
   "source": [
    "idx = 40\n",
    "print(eval_squad_examples[idx].context)\n",
    "print(eval_squad_examples[idx].question)\n",
    "print(eval_squad_examples[idx].answer_text)   \n",
    "print(eval_squad_examples[idx].start_char_idx, eval_squad_examples[idx].start_char_idx+len(eval_squad_examples[idx].answer_text))   \n",
    "\n",
    "pred_context = eval_squad_examples[idx].context\n",
    "pred_question = eval_squad_examples[idx].question\n",
    "\n",
    "pred_data = create_squad_examples_from_arg(pred_question, pred_context)\n",
    "pred_answer = predict_test(korquad_model, pred_data)\n",
    "print(pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding_length= 249\n",
      "26 18\n",
      "(32, 33) (21, 22)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_context = \"\"\"손흥민(토트넘 홋스퍼)으로부터 '세리머니 선물'을 받았던 토트넘 꼬마 팬이 '북런던 더비'에서 깜짝 페널티킥(PK)을 선보였다.\n",
    "영국 매체 '풋볼 런던'은 12일(현지시간) 토트넘과 아스날 간의 잉글랜드 프리미어리그(EPL) 북런던 더비가 끝난 뒤 '어린 토트넘 팬 라일리 키스(Ryley Keys)가 PK를 기록했다'라는 제목의 기사를 보도했다.\n",
    "이날 토트넘은 아스날과의 홈 경기에서 3대 0 승리를 거뒀다. 손흥민은 이번 경기에서 선발로 출전해 PK 유도, 상대 선수 퇴장 유도, 팀의 세 번째 득점 기록 등의 맹활약을 펼친 뒤 후반 72분 교체됐다.\n",
    "토트넘의 꼬마 팬 라일리는 이날 토트넘이 2대 0으로 리드를 잡은 전반전이 끝난 뒤 하프 타임에 모습을 드러냈다.\"\"\"\n",
    "# pred_question = \"라일리가 모습을 드러냈을때 스코어는?\"\n",
    "# pred_question = \"토트넘과 아스널의 최종 스코어는?\"\n",
    "pred_question = \"손흥민은 누구에게 깜짝 선물을 했는가?\"\n",
    "pred_data = create_squad_examples_from_arg(pred_question, pred_context)\n",
    "pred_answer = predict_test(korquad_model, pred_data)\n",
    "print(pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMn6I90a+EqoM9Ks6eBcRWt",
   "collapsed_sections": [],
   "name": "KorQuad_class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1fde406732df4b5b90b7701dc7e4981e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c0ecef646d44a0580cacefa5c3fd9f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3be3c9704e934fb5a3d5847749d398ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7d4d0c48cda4abdb106a6bcfb24359e",
      "placeholder": "​",
      "style": "IPY_MODEL_f58154a65f974e04bcf8af24b2884fdd",
      "value": " 872k/872k [00:00&lt;00:00, 3.17MB/s]"
     }
    },
    "a7d4d0c48cda4abdb106a6bcfb24359e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc7f3c579a324f77811bdd6ad6dd7dc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f8f80478dfca4894ac1ff8c2a082f734",
       "IPY_MODEL_3be3c9704e934fb5a3d5847749d398ce"
      ],
      "layout": "IPY_MODEL_e31de13423d743e68d6c451d23c93cdf"
     }
    },
    "e31de13423d743e68d6c451d23c93cdf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f58154a65f974e04bcf8af24b2884fdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8f80478dfca4894ac1ff8c2a082f734": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fde406732df4b5b90b7701dc7e4981e",
      "max": 871891,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2c0ecef646d44a0580cacefa5c3fd9f2",
      "value": 871891
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
