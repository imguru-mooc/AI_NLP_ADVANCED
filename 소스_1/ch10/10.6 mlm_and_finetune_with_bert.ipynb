{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3_ZCxP-V6p_"
   },
   "source": [
    "# BERT를 사용한 종단 간 마스크 언어 모델링\n",
    "\n",
    "**Author:** [Ankur Singh](https://twitter.com/ankur310794)<br>\n",
    "**Date created:** 2020/09/18<br>\n",
    "**Last modified:** 2020/09/18<br>\n",
    "**Description:** BERT를 사용하여 MLM(Masked Language Model)을 구현하고 IMDB 리뷰 데이터 세트에서 미세 조정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4-b07tZV6qB"
   },
   "source": [
    "## 소개\n",
    "\n",
    "Masked Language Modeling은 빈칸 채우기 작업으로, 모델이 마스크 토큰을 둘러싼 컨텍스트 단어를 사용하여 마스크된 단어가 무엇인지 예측하려고 시도합니다.\n",
    "\n",
    "하나 이상의 마스크 토큰이 포함된 입력의 경우 모델은 각각에 대해 가장 가능성이 높은 대체를 생성합니다.\n",
    "\n",
    "Example:\n",
    "\n",
    "- Input: \"I have watched this [MASK] and it was awesome.\"\n",
    "- Output: \"I have watched this movie and it was awesome.\"\n",
    "\n",
    "마스크된 언어 모델링은 셀프 지도학습 설정(사람이 주석 처리한 레이블 없음)에서 언어 모델을 훈련하는 좋은 방법입니다. 그런 다음 이러한 모델을 미세 조정하여 다양한 지도학습 NLP 작업을 수행할 수 있습니다.\n",
    "\n",
    "이 예제는 BERT 모델을 처음부터 구축하고, 마스크된 언어 모델링 작업으로 훈련시킨 다음, 감정 분류 작업에서 이 모델을 미세 조정하는 방법을 알려줍니다.\n",
    "\n",
    "Keras `TextVectorization`와 `MultiHeadAttention`레이어를 사용하여 BERT Transformer-Encoder 네트워크 아키텍처를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nAC-bkmCV6qC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6VzKSgZV6qC"
   },
   "source": [
    "## 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "p4qo_jc8V6qD"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YZFxCvwV6qD"
   },
   "source": [
    "## 데이터 로드\n",
    "\n",
    "먼저 IMDB 데이터를 다운로드하고 Pandas 데이터 프레임에 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kdX-D8hPV6qD",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 80.2M    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 80.2M    0  160k    0     0   105k      0  0:12:58  0:00:01  0:12:57  105k\n",
      "  0 80.2M    0  624k    0     0   251k      0  0:05:26  0:00:02  0:05:24  251k\n",
      "  1 80.2M    1 1584k    0     0   450k      0  0:03:02  0:00:03  0:02:59  450k\n",
      "  3 80.2M    3 3136k    0     0   701k      0  0:01:57  0:00:04  0:01:53  701k\n",
      "  6 80.2M    6 5744k    0     0  1050k      0  0:01:18  0:00:05  0:01:13 1177k\n",
      " 11 80.2M   11 9056k    0     0  1400k      0  0:00:58  0:00:06  0:00:52 1796k\n",
      " 16 80.2M   16 13.0M    0     0  1788k      0  0:00:45  0:00:07  0:00:38 2551k\n",
      " 23 80.2M   23 18.9M    0     0  2288k      0  0:00:35  0:00:08  0:00:27 3595k\n",
      " 33 80.2M   33 27.0M    0     0  2925k      0  0:00:28  0:00:09  0:00:19 4913k\n",
      " 45 80.2M   45 36.1M    0     0  3530k      0  0:00:23  0:00:10  0:00:13 6231k\n",
      " 53 80.2M   53 43.1M    0     0  3848k      0  0:00:21  0:00:11  0:00:10 7006k\n",
      " 63 80.2M   63 50.9M    0     0  4180k      0  0:00:19  0:00:12  0:00:07 7747k\n",
      " 73 80.2M   73 59.2M    0     0  4503k      0  0:00:18  0:00:13  0:00:05 8249k\n",
      " 84 80.2M   84 68.0M    0     0  4807k      0  0:00:17  0:00:14  0:00:03 8363k\n",
      " 95 80.2M   95 77.0M    0     0  5096k      0  0:00:16  0:00:15  0:00:01 8391k\n",
      "100 80.2M  100 80.2M    0     0  5199k      0  0:00:15  0:00:15 --:--:-- 8795k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qyWph2Q7V6qE"
   },
   "outputs": [],
   "source": [
    "def get_text_list_from_files(files):\n",
    "    text_list = []\n",
    "    for name in files:\n",
    "        with open(name, encoding='UTF-8') as f:\n",
    "            for line in f:\n",
    "                text_list.append(line)\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def get_data_from_text_files(folder_name):\n",
    "\n",
    "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
    "    pos_texts = get_text_list_from_files(pos_files)\n",
    "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
    "    neg_texts = get_text_list_from_files(neg_files)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"review\": pos_texts + neg_texts,\n",
    "            \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),\n",
    "        }\n",
    "    )\n",
    "    df = df.sample(len(df)).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = get_data_from_text_files(\"train\")\n",
    "test_df = get_data_from_text_files(\"test\")\n",
    "\n",
    "all_data = train_df.append(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKJTteNjV6qE"
   },
   "source": [
    "## 데이터 세트 준비\n",
    "\n",
    "`TextVectorization` 레이어를 사용 하여 텍스트를 정수 토큰 ID로 벡터화합니다. 문자열 배치를 토큰 인덱스 시퀀스(순서대로 하나의 샘플 = 정수 토큰 인덱스의 1D 배열) 또는 조밀한 표현(하나의 샘플 = 정렬되지 않은 토큰 세트를 인코딩하는 부동 소수점 값의 1D 배열)으로 변환합니다.\n",
    "\n",
    "아래에서는 3개의 전처리 기능을 정의합니다.\n",
    "\n",
    "1.  `get_vectorize_layer`함수는 `TextVectorization`레이어를 만듭니다.\n",
    "2.  `encode` 함수는 원시 텍스트를 정수 토큰 ID로 인코딩합니다.\n",
    "3.  `get_masked_input_and_labels`함수는 입력 토큰 ID를 마스킹합니다. 무작위로 각 시퀀스의 모든 입력 토큰의 15%를 마스킹합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Ruf214BXV6qE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
    "    \"\"\"텍스트 벡터화 레이어 구축\n",
    "\n",
    "    Args:\n",
    "      texts (list): 문자열 목록, 즉 입력 텍스트\n",
    "      vocab_size (int): 어휘 크기\n",
    "      max_seq (int): 최대 시퀀스 길이.\n",
    "      special_tokens (list, optional): 특수 토큰 목록입니다. 기본값은 ['[MASK]']입니다.\n",
    "\n",
    "    Returns:\n",
    "        layers.Layer: TextVectorization Keras 레이어 반환\n",
    "    \"\"\"\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=custom_standardization,\n",
    "        output_sequence_length=max_seq,\n",
    "    )\n",
    "    vectorize_layer.adapt(texts)\n",
    "\n",
    "    # 어휘에 마스크 토큰 삽입\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "    return vectorize_layer\n",
    "\n",
    "\n",
    "vectorize_layer = get_vectorize_layer(\n",
    "    all_data.review.values.tolist(),\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "# 마스크된 언어 모델에 대한 마스크 토큰 ID 가져오기\n",
    "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
    "\n",
    "def encode(texts):\n",
    "    encoded_texts = vectorize_layer(texts)\n",
    "    return encoded_texts.numpy()\n",
    "\n",
    "\n",
    "def get_masked_input_and_labels(encoded_texts):\n",
    "    # 15% BERT 마스킹\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "    # 특수 토큰을 마스킹하지 마십시오.\n",
    "    inp_mask[encoded_texts <= 2] = False\n",
    "    # 기본적으로 대상을 -1로 설정합니다. 무시를 의미합니다.\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # 마스킹된 토큰에 대한 레이블 설정\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # 입력 준비\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    # 90%의 토큰에 대한 마지막 토큰인 [MASK]에 입력을 설정합니다.\n",
    "    # 이것은 10%를 그대로 두는 것을 의미합니다.\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "    encoded_texts_masked[\n",
    "        inp_mask_2mask\n",
    "    ] = mask_token_id  # 마스크 토큰은 dict의 마지막입니다.\n",
    "\n",
    "    # 10%를 임의의 토큰으로 설정\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
    "        3, mask_token_id, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # .fit() 메서드에 전달할 sample_weights 준비\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # y_labels는 encode_texts, 즉 입력 토큰과 동일합니다.\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    return encoded_texts_masked, y_labels, sample_weights\n",
    "\n",
    "\n",
    "# 훈련을 위한 25000개의 예제가 있습니다.\n",
    "x_train = encode(train_df.review.values)  # 벡터라이저로 리뷰 인코딩\n",
    "y_train = train_df.sentiment.values\n",
    "train_classifier_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(1000)\n",
    "    .batch(config.BATCH_SIZE)\n",
    ")\n",
    "\n",
    "# 테스트를 위한 25000개의 예제가 있습니다.\n",
    "x_test = encode(test_df.review.values)\n",
    "y_test = test_df.sentiment.values\n",
    "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
    "    config.BATCH_SIZE\n",
    ")\n",
    "\n",
    "# 종단 간 모델 입력을 위한 데이터 세트 구축(마지막에 사용됨)\n",
    "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_df.review.values, y_test)\n",
    ").batch(config.BATCH_SIZE)\n",
    "\n",
    "# 마스킹된 언어 모델에 대한 데이터 준비\n",
    "x_all_review = encode(all_data.review.values)\n",
    "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
    "    x_all_review\n",
    ")\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_train, y_masked_labels, sample_weights)\n",
    ")\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZvUuWXRV6qF"
   },
   "source": [
    "## 마스크 언어 모델링을 위한 BERT 모델(Pretraining Model) 생성\n",
    "\n",
    "레이어 를 사용하여 BERT와 같은 사전 학습 모델 아키텍처를 생성합니다 `MultiHeadAttention`. 토큰 ID를 입력(마스킹된 토큰 포함)으로 사용하고 마스크된 입력 토큰의 올바른 ID를 예측합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "rW0CI35wV6qG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)     (None, 256, 128)     3840000     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 256, 128)    0           ['word_embedding[0][0]']         \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattention (  (None, 256, 128)    66048       ['tf.__operators__.add[0][0]',   \n",
      " MultiHeadAttention)                                              'tf.__operators__.add[0][0]',   \n",
      "                                                                  'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dropout  (None, 256, 128)    0           ['encoder_0/multiheadattention[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 256, 128)    0           ['tf.__operators__.add[0][0]',   \n",
      " mbda)                                                            'encoder_0/att_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormalizati  (None, 256, 128)    256         ['tf.__operators__.add_1[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)     (None, 256, 128)     33024       ['encoder_0/att_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dropout  (None, 256, 128)    0           ['encoder_0/ffn[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 256, 128)    0           ['encoder_0/att_layernormalizatio\n",
      " mbda)                                                           n[0][0]',                        \n",
      "                                                                  'encoder_0/ffn_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormalizati  (None, 256, 128)    256         ['tf.__operators__.add_2[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)                (None, 256, 30000)   3870000     ['encoder_0/ffn_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,809,584\n",
      "Trainable params: 7,809,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def bert_module(query, key, value, i):\n",
    "    # Multi headed self-attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "        name=\"encoder_{}/multiheadattention\".format(i),\n",
    "    )(query, key, value)\n",
    "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
    "        attention_output\n",
    "    )\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ],\n",
    "        name=\"encoder_{}/ffn\".format(i),\n",
    "    )\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
    "        ffn_output\n",
    "    )\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
    "    )(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "\n",
    "def get_pos_encoding_matrix(max_len, d_emb):\n",
    "    pos_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
    "            if pos != 0\n",
    "            else np.zeros(d_emb)\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(tf.keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        if len(inputs) == 3:\n",
    "            features, labels, sample_weight = inputs\n",
    "        else:\n",
    "            features, labels = inputs\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
    "\n",
    "        # 그라디언트 계산\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # 가중치 업데이트\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # 메트릭 계산\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # 메트릭 이름을 현재 값으로 매핑하는 dict 반환\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # `reset_states()`가 될 수 있도록 `Metric` 객체를 여기에 나열합니다.\n",
    "        # 각 Epoch 시작 시 자동으로 호출됨\n",
    "        # 또는 `evaluate()`의 시작 부분에서.\n",
    "        # 이 속성을 구현하지 않으면 다음을 호출해야 합니다.\n",
    "        # 선택한 시간에 `reset_states()`\n",
    "        return [loss_tracker]\n",
    "\n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "\n",
    "    word_embeddings = layers.Embedding(\n",
    "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
    "    )(inputs)\n",
    "    position_embeddings = layers.Embedding(\n",
    "        input_dim=config.MAX_LEN,\n",
    "        output_dim=config.EMBED_DIM,\n",
    "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
    "        name=\"position_embedding\",\n",
    "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
    "        encoder_output\n",
    "    )\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model\n",
    "\n",
    "\n",
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "token2id = {y: x for x, y in id2token.items()}\n",
    "\n",
    "\n",
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=5):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        return id2token[id]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
    "        masked_index = masked_index[1]\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "\n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = values[i]\n",
    "            tokens = np.copy(sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            result = {\n",
    "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)\n",
    "\n",
    "\n",
    "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
    "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
    "\n",
    "bert_masked_model = create_masked_language_bert_model()\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJwQ7K9oV6qG"
   },
   "source": [
    "## 훈련과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Fizq2uaPV6qH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 6.9901{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.048747342}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'i have watched this a and it was awesome',\n",
      " 'probability': 0.043583322}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.03734729}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.032652}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'of',\n",
      " 'prediction': 'i have watched this of and it was awesome',\n",
      " 'probability': 0.029586175}\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 6.9901\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 6.4395{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.15612747}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'film',\n",
      " 'prediction': 'i have watched this film and it was awesome',\n",
      " 'probability': 0.035530604}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'was',\n",
      " 'prediction': 'i have watched this was and it was awesome',\n",
      " 'probability': 0.019993294}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'of',\n",
      " 'prediction': 'i have watched this of and it was awesome',\n",
      " 'probability': 0.017330125}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'at',\n",
      " 'prediction': 'i have watched this at and it was awesome',\n",
      " 'probability': 0.015978023}\n",
      "1563/1563 [==============================] - 61s 39ms/step - loss: 6.4395\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 5.8675{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.24962868}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'film',\n",
      " 'prediction': 'i have watched this film and it was awesome',\n",
      " 'probability': 0.21191935}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'one',\n",
      " 'prediction': 'i have watched this one and it was awesome',\n",
      " 'probability': 0.047277953}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'show',\n",
      " 'prediction': 'i have watched this show and it was awesome',\n",
      " 'probability': 0.02840738}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'is',\n",
      " 'prediction': 'i have watched this is and it was awesome',\n",
      " 'probability': 0.027484253}\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 5.8675\n",
      "Epoch 4/5\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 5.4102{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.24972202}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'film',\n",
      " 'prediction': 'i have watched this film and it was awesome',\n",
      " 'probability': 0.15966913}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'one',\n",
      " 'prediction': 'i have watched this one and it was awesome',\n",
      " 'probability': 0.15867439}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'series',\n",
      " 'prediction': 'i have watched this series and it was awesome',\n",
      " 'probability': 0.01493295}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'show',\n",
      " 'prediction': 'i have watched this show and it was awesome',\n",
      " 'probability': 0.013808772}\n",
      "1563/1563 [==============================] - 59s 38ms/step - loss: 5.4103\n",
      "Epoch 5/5\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 4.9044{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.37940568}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'film',\n",
      " 'prediction': 'i have watched this film and it was awesome',\n",
      " 'probability': 0.25397527}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'one',\n",
      " 'prediction': 'i have watched this one and it was awesome',\n",
      " 'probability': 0.038779765}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'series',\n",
      " 'prediction': 'i have watched this series and it was awesome',\n",
      " 'probability': 0.018691126}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'time',\n",
      " 'prediction': 'i have watched this time and it was awesome',\n",
      " 'probability': 0.015949989}\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 4.9043\n"
     ]
    }
   ],
   "source": [
    "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
    "bert_masked_model.save(\"bert_mlm_imdb.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1ERRjisV6qH"
   },
   "source": [
    "## 감정 분류 모델 미세 조정\n",
    "\n",
    "감정 분류의 다운스트림 작업에서 자체 지도 모델을 미세 조정할 것입니다. 이를 위해 `Dense`사전 훈련된 BERT 기능 위에 풀링 계층과 계층을 추가하여 분류기를 생성해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "OQIJ-lEoV6qH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 256)]             0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 256, 128)          3939584   \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,947,905\n",
      "Trainable params: 8,321\n",
      "Non-trainable params: 3,939,584\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.6947 - accuracy: 0.5860 - val_loss: 0.6452 - val_accuracy: 0.6281\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.6542 - accuracy: 0.6248 - val_loss: 0.6414 - val_accuracy: 0.6333\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.6518 - accuracy: 0.6298 - val_loss: 0.6284 - val_accuracy: 0.6453\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.6472 - accuracy: 0.6306 - val_loss: 0.6269 - val_accuracy: 0.6474\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.6365 - accuracy: 0.6388 - val_loss: 0.6264 - val_accuracy: 0.6485\n",
      "Epoch 1/5\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.4543 - accuracy: 0.7805 - val_loss: 0.3704 - val_accuracy: 0.8372\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.2751 - accuracy: 0.8840 - val_loss: 0.3820 - val_accuracy: 0.8513\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.1483 - accuracy: 0.9427 - val_loss: 0.4128 - val_accuracy: 0.8541\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.0649 - accuracy: 0.9759 - val_loss: 0.6819 - val_accuracy: 0.8346\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.0353 - accuracy: 0.9872 - val_loss: 0.7381 - val_accuracy: 0.8226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16c6b345dc0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전 훈련된 bert 모델 불러오기\n",
    "mlm_model = keras.models.load_model(\n",
    "    \"bert_mlm_imdb.h5\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n",
    ")\n",
    "pretrained_bert_model = tf.keras.Model(\n",
    "    mlm_model.input, mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output\n",
    ")\n",
    "\n",
    "# 동결\n",
    "pretrained_bert_model.trainable = False\n",
    "\n",
    "\n",
    "def create_classifier_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "    sequence_output = pretrained_bert_model(inputs)\n",
    "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
    "    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "    classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    classifer_model.compile(\n",
    "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return classifer_model\n",
    "\n",
    "\n",
    "classifer_model = create_classifier_bert_model()\n",
    "classifer_model.summary()\n",
    "\n",
    "# 고정된 BERT 단계로 분류기 훈련\n",
    "classifer_model.fit(\n",
    "    train_classifier_ds,\n",
    "    epochs=5,\n",
    "    validation_data=test_classifier_ds,\n",
    ")\n",
    "\n",
    "# 미세 조정을 위해 BERT 모델 고정 해제\n",
    "pretrained_bert_model.trainable = True\n",
    "optimizer = keras.optimizers.Adam()\n",
    "classifer_model.compile(\n",
    "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "classifer_model.fit(\n",
    "    train_classifier_ds,\n",
    "    epochs=5,\n",
    "    validation_data=test_classifier_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TF1rx2-QV6qH"
   },
   "source": [
    "## 종단 간 모델 생성 및 평가\n",
    "\n",
    "모델을 배포하려는 경우 프로덕션 환경에서 사전 처리 논리를 다시 구현할 필요가 없도록 사전 처리 파이프라인이 이미 포함되어 있는 것이 가장 좋습니다. `TextVectorization`레이어 를 통합하는 종단 간 모델을 만들고 평가해 보겠습니다. 우리 모델은 원시 문자열을 입력으로 받아들입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ATUq44ejV6qH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 7ms/step - loss: 0.7381 - accuracy: 0.8226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7381284236907959, 0.8225600123405457]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_end_to_end(model):\n",
    "    inputs_string = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    indices = vectorize_layer(inputs_string)\n",
    "    outputs = model(indices)\n",
    "    end_to_end_model = keras.Model(inputs_string, outputs, name=\"end_to_end_model\")\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    end_to_end_model.compile(\n",
    "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return end_to_end_model\n",
    "\n",
    "\n",
    "end_to_end_classification_model = get_end_to_end(classifer_model)\n",
    "end_to_end_classification_model.evaluate(test_raw_classifier_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlm_and_finetune_with_bert",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
