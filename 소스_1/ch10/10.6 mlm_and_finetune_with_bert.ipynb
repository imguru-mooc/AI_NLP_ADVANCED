{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3_ZCxP-V6p_"
   },
   "source": [
    "# BERT를 사용한 종단 간 마스크 언어 모델링\n",
    "\n",
    "**Author:** [Ankur Singh](https://twitter.com/ankur310794)<br>\n",
    "**Date created:** 2020/09/18<br>\n",
    "**Last modified:** 2020/09/18<br>\n",
    "**Description:** BERT를 사용하여 MLM(Masked Language Model)을 구현하고 IMDB 리뷰 데이터 세트에서 미세 조정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4-b07tZV6qB"
   },
   "source": [
    "## 소개\n",
    "\n",
    "Masked Language Modeling은 빈칸 채우기 작업으로, 모델이 마스크 토큰을 둘러싼 컨텍스트 단어를 사용하여 마스크된 단어가 무엇인지 예측하려고 시도합니다.\n",
    "\n",
    "하나 이상의 마스크 토큰이 포함된 입력의 경우 모델은 각각에 대해 가장 가능성이 높은 대체를 생성합니다.\n",
    "\n",
    "Example:\n",
    "\n",
    "- Input: \"I have watched this [MASK] and it was awesome.\"\n",
    "- Output: \"I have watched this movie and it was awesome.\"\n",
    "\n",
    "마스크된 언어 모델링은 셀프 지도학습(SSL) 설정(사람이 주석 처리한 레이블 없음)에서 언어 모델을 훈련하는 좋은 방법입니다. 그런 다음 이러한 모델을 미세 조정하여 다양한 지도학습 NLP 작업을 수행할 수 있습니다.\n",
    "\n",
    "이 예제는 BERT 모델을 처음부터 구축하고, 마스크된 언어 모델링 작업으로 훈련시킨 다음, 감정 분류 작업에서 이 모델을 미세 조정하는 방법을 알려줍니다.\n",
    "\n",
    "Keras `TextVectorization`와 `MultiHeadAttention`레이어를 사용하여 BERT Transformer-Encoder 네트워크 아키텍처를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nAC-bkmCV6qC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6VzKSgZV6qC"
   },
   "source": [
    "## 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "p4qo_jc8V6qD"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YZFxCvwV6qD"
   },
   "source": [
    "## 데이터 로드\n",
    "\n",
    "먼저 IMDB 데이터를 다운로드하고 Pandas 데이터 프레임에 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "kdX-D8hPV6qD",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 80.2M    0 16384    0     0  19220      0  1:12:56 --:--:--  1:12:56 19230\n",
      "  0 80.2M    0  224k    0     0   115k      0  0:11:52  0:00:01  0:11:51  115k\n",
      "  0 80.2M    0  560k    0     0   190k      0  0:07:11  0:00:02  0:07:09  190k\n",
      "  1 80.2M    1 1104k    0     0   279k      0  0:04:54  0:00:03  0:04:51  279k\n",
      "  2 80.2M    2 1952k    0     0   396k      0  0:03:27  0:00:04  0:03:23  396k\n",
      "  3 80.2M    3 3216k    0     0   545k      0  0:02:30  0:00:05  0:02:25  633k\n",
      "  6 80.2M    6 5008k    0     0   726k      0  0:01:53  0:00:06  0:01:47  967k\n",
      "  9 80.2M    9 7504k    0     0   950k      0  0:01:26  0:00:07  0:01:19 1403k\n",
      " 13 80.2M   13 10.6M    0     0  1228k      0  0:01:06  0:00:08  0:00:58 1988k\n",
      " 18 80.2M   18 14.6M    0     0  1522k      0  0:00:53  0:00:09  0:00:44 2652k\n",
      " 24 80.2M   24 20.0M    0     0  1890k      0  0:00:43  0:00:10  0:00:33 3497k\n",
      " 34 80.2M   34 27.4M    0     0  2368k      0  0:00:34  0:00:11  0:00:23 4642k\n",
      " 46 80.2M   46 37.1M    0     0  2947k      0  0:00:27  0:00:12  0:00:15 6093k\n",
      " 61 80.2M   61 49.0M    0     0  3629k      0  0:00:22  0:00:13  0:00:09 7935k\n",
      " 82 80.2M   82 65.9M    0     0  4548k      0  0:00:18  0:00:14  0:00:04 10.2M\n",
      "100 80.2M  100 80.2M    0     0  5276k      0  0:00:15  0:00:15 --:--:-- 12.7M\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "qyWph2Q7V6qE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  I have just watched the movie for the first ti...          1\n",
      "1  Matt Saunders (Luke Wilson) thinks he has foun...          1\n",
      "2  Why can't more directors these days create hor...          0\n",
      "3  There's a legion of Mick Garris haters out the...          0\n",
      "4  while being one of the \"stars\" of this film do...          1\n"
     ]
    }
   ],
   "source": [
    "def get_text_list_from_files(files):\n",
    "    text_list = []\n",
    "    for name in files:\n",
    "        with open(name, encoding='UTF-8') as f:\n",
    "            for line in f:\n",
    "                text_list.append(line)\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def get_data_from_text_files(folder_name):\n",
    "\n",
    "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
    "    pos_texts = get_text_list_from_files(pos_files)\n",
    "#     print(pos_texts)\n",
    "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
    "    neg_texts = get_text_list_from_files(neg_files)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"review\": pos_texts + neg_texts,\n",
    "            \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),\n",
    "        }\n",
    "    )\n",
    "    df = df.sample(len(df)).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = get_data_from_text_files(\"train\")\n",
    "test_df = get_data_from_text_files(\"test\")\n",
    "\n",
    "print(train_df.head())\n",
    "\n",
    "all_data = train_df.append(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKJTteNjV6qE"
   },
   "source": [
    "## 데이터 세트 준비\n",
    "\n",
    "`TextVectorization` 레이어를 사용 하여 텍스트를 정수 토큰 ID로 벡터화합니다. 문자열 배치를 토큰 인덱스 시퀀스(순서대로 하나의 샘플 = 정수 토큰 인덱스의 1D 배열) 또는 조밀한 표현(하나의 샘플 = 정렬되지 않은 토큰 세트를 인코딩하는 부동 소수점 값의 1D 배열)으로 변환합니다.\n",
    "\n",
    "아래에서는 3개의 전처리 기능을 정의합니다.\n",
    "\n",
    "1.  `get_vectorize_layer`함수는 `TextVectorization`레이어를 만듭니다.\n",
    "2.  `encode` 함수는 원시 텍스트를 정수 토큰 ID로 인코딩합니다.\n",
    "3.  `get_masked_input_and_labels`함수는 입력 토큰 ID를 마스킹합니다. 무작위로 각 시퀀스의 모든 입력 토큰의 15%를 마스킹합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "Ruf214BXV6qE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
    "    \"\"\"텍스트 벡터화 레이어 구축\n",
    "\n",
    "    Args:\n",
    "      texts (list): 문자열 목록, 즉 입력 텍스트\n",
    "      vocab_size (int): 어휘 크기\n",
    "      max_seq (int): 최대 시퀀스 길이.\n",
    "      special_tokens (list, optional): 특수 토큰 목록입니다. 기본값은 ['[MASK]']입니다.\n",
    "\n",
    "    Returns:\n",
    "        layers.Layer: TextVectorization Keras 레이어 반환\n",
    "    \"\"\"\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=custom_standardization,\n",
    "        output_sequence_length=max_seq,\n",
    "    )\n",
    "    vectorize_layer.adapt(texts)\n",
    "\n",
    "    # 어휘에 마스크 토큰 삽입\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "#     print(vocab)\n",
    "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
    "#     print(vocab)\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "    return vectorize_layer\n",
    "\n",
    "\n",
    "vectorize_layer = get_vectorize_layer(\n",
    "    all_data.review.values.tolist(),\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "# 마스크된 언어 모델에 대한 마스크 토큰 ID 가져오기\n",
    "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
    "# print(mask_token_id)\n",
    "\n",
    "def encode(texts):\n",
    "    encoded_texts = vectorize_layer(texts)\n",
    "    return encoded_texts.numpy()\n",
    "\n",
    "\n",
    "def get_masked_input_and_labels(encoded_texts):\n",
    "    # 15% BERT 마스킹\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "    # 특수 토큰을 마스킹하지 마십시오.\n",
    "    inp_mask[encoded_texts <= 2] = False\n",
    "    # 기본적으로 대상을 -1로 설정합니다. 무시를 의미합니다.\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # 마스킹된 토큰에 대한 레이블 설정\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # 입력 준비\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    # 90%의 토큰에 대한 마지막 토큰인 [MASK]에 입력을 설정합니다.\n",
    "    # 이것은 10%를 그대로 두는 것을 의미합니다.\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "    encoded_texts_masked[\n",
    "        inp_mask_2mask\n",
    "    ] = mask_token_id  # 마스크 토큰은 dict의 마지막입니다.\n",
    "\n",
    "    # 10%를 임의의 토큰으로 설정\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
    "        3, mask_token_id, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # .fit() 메서드에 전달할 sample_weights 준비\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # y_labels는 encode_texts, 즉 입력 토큰과 동일합니다.\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    return encoded_texts_masked, y_labels, sample_weights\n",
    "\n",
    "\n",
    "# 훈련을 위한 25000개의 예제가 있습니다.\n",
    "x_train = encode(train_df.review.values)  # 벡터라이저로 리뷰 인코딩\n",
    "# print(x_train.shape)\n",
    "\n",
    "y_train = train_df.sentiment.values\n",
    "# print(y_train.shape)\n",
    "train_classifier_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(1000)\n",
    "    .batch(config.BATCH_SIZE)\n",
    ")\n",
    "\n",
    "# 테스트를 위한 25000개의 예제가 있습니다.\n",
    "x_test = encode(test_df.review.values)\n",
    "y_test = test_df.sentiment.values\n",
    "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
    "    config.BATCH_SIZE\n",
    ")\n",
    "\n",
    "# 종단 간 모델 입력을 위한 데이터 세트 구축(마지막에 사용됨)\n",
    "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_df.review.values, y_test)\n",
    ").batch(config.BATCH_SIZE)\n",
    "\n",
    "# 마스킹된 언어 모델에 대한 데이터 준비\n",
    "x_all_review = encode(all_data.review.values)\n",
    "# print(x_all_review.shape)\n",
    "\n",
    "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
    "    x_all_review\n",
    ")\n",
    "\n",
    "# print(x_masked_train[0])\n",
    "# print(y_masked_labels[0])\n",
    "# print(sample_weights[0])\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_train, y_masked_labels, sample_weights)\n",
    ")\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'this is really a new low in entertainment even though there are a lot worse movies out  in the gangster  drug scene genre it is hard to have a convincing storyline this movies does not i mean sebastians motives for example couldnt be more far fetched and worn out clich\\xc3\\xa9 then you would also need a setting of character relationships that is believable this movie does not   sure tristan is drawn away from his family but why was that again whats the deal with his father again that he has to ask permission to go out at his age interesting picture though to ask about the lack and need of rebellious behavior of kids in upper class family but this movie does not go in this direction even though there would be the potential judging by the random backflashes wasnt he already down and out why does he do it again   so there are some interesting questions brought up here for a solid socially critic drama but then again this movie is just not because of focusing on \"cool\" production techniques and special effects an not giving the characters a moment to reflect and most of all forcing the story along the path where they want it to be and not paying attention to let the story breath and naturally evolve   it wants to be a drama to not glorify abuse of substances and violence would be political incorrect these days wouldnt it but on the other hand it is nothing more then a cheap action movie like there are so so many out there with an average set of actors and a vinnie jones who is managing to not totally ruin whats left of his reputation by doing what he always does  so all in all i  just  cant recommend it  1 for vinnie and 2 for the editing', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sent = \"\"\"This is really a new low in entertainment. Even though there are a lot worse movies out.<br /><br />In the Gangster / Drug scene genre it is hard to have a convincing storyline (this movies does not, i mean Sebastians motives for example couldn't be more far fetched and worn out cliché.) Then you would also need a setting of character relationships that is believable (this movie does not.) <br /><br />Sure Tristan is drawn away from his family but why was that again? what's the deal with his father again that he has to ask permission to go out at his age? interesting picture though to ask about the lack and need of rebellious behavior of kids in upper class family. But this movie does not go in this direction. Even though there would be the potential judging by the random Backflashes. Wasn't he already down and out, why does he do it again? <br /><br />So there are some interesting questions brought up here for a solid socially critic drama (but then again, this movie is just not, because of focusing on \"cool\" production techniques and special effects an not giving the characters a moment to reflect and most of all forcing the story along the path where they want it to be and not paying attention to let the story breath and naturally evolve.) <br /><br />It wants to be a drama to not glorify abuse of substances and violence (would be political incorrect these days, wouldn't it?) but on the other hand it is nothing more then a cheap action movie (like there are so so many out there) with an average set of actors and a Vinnie Jones who is managing to not totally ruin what's left of his reputation by doing what he always does.<br /><br />So all in all i .. just ... can't recommend it.<br /><br />1 for Vinnie and 2 for the editing.\"\"\"\n",
    "\n",
    "sent = custom_standardization(sent)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22131])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randint( 3, 29999, 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]\n",
      " [20 21 22 23 24 25 26 27 28 29]\n",
      " [30 31 32 33 34 35 36 37 38 39]\n",
      " [40 41 42 43 44 45 46 47 48 49]]\n",
      "[[False False  True False False False False  True False False]\n",
      " [False  True False False False False False False False False]\n",
      " [False False False False False False False False  True False]\n",
      " [False  True False  True False  True False False False False]\n",
      " [False False False  True False False False False False False]]\n",
      "[[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1]]\n",
      "[[-1 -1 -1 -1 -1 -1 -1  7 -1 -1]\n",
      " [-1 11 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1 -1 28 -1]\n",
      " [-1 31 -1 33 -1 35 -1 -1 -1 -1]\n",
      " [-1 -1 -1 43 -1 -1 -1 -1 -1 -1]]\n",
      "[[    0     1     2     3     4     5     6 29999     8     9]\n",
      " [   10 29999    12    13    14    15    16    17    18    19]\n",
      " [   20    21    22    23    24    25    26    27 29999    29]\n",
      " [   30 29999    32    33    34 29999    36    37    38    39]\n",
      " [   40    41    42 29999    44    45    46    47    48    49]]\n",
      "[[    0     1     2     3     4     5     6 29999     8     9]\n",
      " [   10 22526    12    13    14    15    16    17    18    19]\n",
      " [   20    21    22    23    24    25    26    27 29999    29]\n",
      " [   30 29999    32    33    34  5217    36    37    38    39]\n",
      " [   40    41    42 29999    44    45    46    47    48    49]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts = np.arange(50).reshape(5,10)\n",
    "print(encoded_texts)\n",
    "inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "print(inp_mask)\n",
    "inp_mask[encoded_texts <= 2] = False\n",
    "# 기본적으로 대상을 -1로 설정합니다. 무시를 의미합니다.\n",
    "labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "print(labels)\n",
    "    # 마스킹된 토큰에 대한 레이블 설정\n",
    "labels[inp_mask] = encoded_texts[inp_mask]\n",
    "print(labels)\n",
    "\n",
    "encoded_texts_masked = np.copy(encoded_texts)\n",
    "# 90%의 토큰에 대한 마지막 토큰인 [MASK]에 입력을 설정합니다.\n",
    "# 이것은 10%를 그대로 두는 것을 의미합니다.\n",
    "mask_token_id = 29999\n",
    "\n",
    "inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "encoded_texts_masked[\n",
    "    inp_mask_2mask\n",
    "] = mask_token_id \n",
    "print(encoded_texts_masked)\n",
    "\n",
    "# 10%를 임의의 토큰으로 설정\n",
    "inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
    "    3, mask_token_id, inp_mask_2random.sum()\n",
    ")\n",
    "print(encoded_texts_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZvUuWXRV6qF"
   },
   "source": [
    "## 마스크 언어 모델링을 위한 BERT 모델(Pretraining Model) 생성\n",
    "\n",
    "레이어 를 사용하여 BERT와 같은 사전 학습 모델 아키텍처를 생성합니다 `MultiHeadAttention`. 토큰 ID를 입력(마스킹된 토큰 포함)으로 사용하고 마스크된 입력 토큰의 올바른 ID를 예측합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "rW0CI35wV6qG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)     (None, 256, 128)     3840000     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 256, 128)    0           ['word_embedding[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattention (  (None, 256, 128)    66048       ['tf.__operators__.add_3[0][0]', \n",
      " MultiHeadAttention)                                              'tf.__operators__.add_3[0][0]', \n",
      "                                                                  'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dropout  (None, 256, 128)    0           ['encoder_0/multiheadattention[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 256, 128)    0           ['tf.__operators__.add_3[0][0]', \n",
      " mbda)                                                            'encoder_0/att_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormalizati  (None, 256, 128)    256         ['tf.__operators__.add_4[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)     (None, 256, 128)     33024       ['encoder_0/att_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dropout  (None, 256, 128)    0           ['encoder_0/ffn[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 256, 128)    0           ['encoder_0/att_layernormalizatio\n",
      " mbda)                                                           n[0][0]',                        \n",
      "                                                                  'encoder_0/ffn_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormalizati  (None, 256, 128)    256         ['tf.__operators__.add_5[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)                (None, 256, 30000)   3870000     ['encoder_0/ffn_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,809,584\n",
      "Trainable params: 7,809,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def bert_module(query, key, value, i):\n",
    "    # Multi headed self-attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "        name=\"encoder_{}/multiheadattention\".format(i),\n",
    "    )(query, key, value)\n",
    "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
    "        attention_output\n",
    "    )\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ],\n",
    "        name=\"encoder_{}/ffn\".format(i),\n",
    "    )\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
    "        ffn_output\n",
    "    )\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
    "    )(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "\n",
    "def get_pos_encoding_matrix(max_len, d_emb):\n",
    "    pos_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
    "            if pos != 0\n",
    "            else np.zeros(d_emb)\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(tf.keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        if len(inputs) == 3:\n",
    "            features, labels, sample_weight = inputs\n",
    "        else:\n",
    "            features, labels = inputs\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
    "\n",
    "        # 그라디언트 계산\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # 가중치 업데이트\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # 메트릭 계산\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # 메트릭 이름을 현재 값으로 매핑하는 dict 반환\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # `reset_states()`가 될 수 있도록 `Metric` 객체를 여기에 나열합니다.\n",
    "        # 각 Epoch 시작 시 자동으로 호출됨\n",
    "        # 또는 `evaluate()`의 시작 부분에서.\n",
    "        # 이 속성을 구현하지 않으면 다음을 호출해야 합니다.\n",
    "        # 선택한 시간에 `reset_states()`\n",
    "        return [loss_tracker]\n",
    "\n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "\n",
    "    word_embeddings = layers.Embedding(\n",
    "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
    "    )(inputs)\n",
    "    position_embeddings = layers.Embedding(\n",
    "        input_dim=config.MAX_LEN,\n",
    "        output_dim=config.EMBED_DIM,\n",
    "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
    "        name=\"position_embedding\",\n",
    "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
    "        encoder_output\n",
    "    )\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model\n",
    "\n",
    "\n",
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "token2id = {y: x for x, y in id2token.items()}\n",
    "\n",
    "\n",
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=5):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        return id2token[id]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
    "        masked_index = masked_index[1]\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "\n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = values[i]\n",
    "            tokens = np.copy(sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            result = {\n",
    "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)\n",
    "\n",
    "\n",
    "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
    "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
    "\n",
    "bert_masked_model = create_masked_language_bert_model()\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJwQ7K9oV6qG"
   },
   "source": [
    "## 훈련과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "Fizq2uaPV6qH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 6.9939{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.07171735}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.06905679}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.042834535}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'i have watched this a and it was awesome',\n",
      " 'probability': 0.027843794}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'film',\n",
      " 'prediction': 'i have watched this film and it was awesome',\n",
      " 'probability': 0.027195461}\n",
      "1563/1563 [==============================] - 108s 69ms/step - loss: 6.9939\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 6.3608{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.45618543}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'film',\n",
      " 'prediction': 'i have watched this film and it was awesome',\n",
      " 'probability': 0.1503488}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'one',\n",
      " 'prediction': 'i have watched this one and it was awesome',\n",
      " 'probability': 0.057965755}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'is',\n",
      " 'prediction': 'i have watched this is and it was awesome',\n",
      " 'probability': 0.023262572}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'was',\n",
      " 'prediction': 'i have watched this was and it was awesome',\n",
      " 'probability': 0.011731229}\n",
      "1563/1563 [==============================] - 118s 75ms/step - loss: 6.3608\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 5.7453{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.4403137}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'film',\n",
      " 'prediction': 'i have watched this film and it was awesome',\n",
      " 'probability': 0.13873687}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'one',\n",
      " 'prediction': 'i have watched this one and it was awesome',\n",
      " 'probability': 0.06650577}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'series',\n",
      " 'prediction': 'i have watched this series and it was awesome',\n",
      " 'probability': 0.018903831}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'episode',\n",
      " 'prediction': 'i have watched this episode and it was awesome',\n",
      " 'probability': 0.013688615}\n",
      "1563/1563 [==============================] - 92s 59ms/step - loss: 5.7453\n",
      "Epoch 4/5\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 5.1355{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.5558211}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'film',\n",
      " 'prediction': 'i have watched this film and it was awesome',\n",
      " 'probability': 0.14854167}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'one',\n",
      " 'prediction': 'i have watched this one and it was awesome',\n",
      " 'probability': 0.026688484}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'review',\n",
      " 'prediction': 'i have watched this review and it was awesome',\n",
      " 'probability': 0.01680535}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'dvd',\n",
      " 'prediction': 'i have watched this dvd and it was awesome',\n",
      " 'probability': 0.012647577}\n",
      "1563/1563 [==============================] - 84s 53ms/step - loss: 5.1354\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 4.7698{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.62535286}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'film',\n",
      " 'prediction': 'i have watched this film and it was awesome',\n",
      " 'probability': 0.12598518}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'one',\n",
      " 'prediction': 'i have watched this one and it was awesome',\n",
      " 'probability': 0.0277566}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'review',\n",
      " 'prediction': 'i have watched this review and it was awesome',\n",
      " 'probability': 0.015462466}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'time',\n",
      " 'prediction': 'i have watched this time and it was awesome',\n",
      " 'probability': 0.012839394}\n",
      "1563/1563 [==============================] - 123s 79ms/step - loss: 4.7698\n"
     ]
    }
   ],
   "source": [
    "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
    "bert_masked_model.save(\"bert_mlm_imdb.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07852171 0.05681154 0.04478914 0.03614069 0.0188529 ]\n",
      "{'input_text': 'this movie did not even [mask] close to being boring',\n",
      " 'predicted mask token': 'much',\n",
      " 'prediction': 'this movie did not even much close to being boring',\n",
      " 'probability': 0.078521706}\n",
      "{'input_text': 'this movie did not even [mask] close to being boring',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'this movie did not even a close to being boring',\n",
      " 'probability': 0.056811538}\n",
      "{'input_text': 'this movie did not even [mask] close to being boring',\n",
      " 'predicted mask token': 'hard',\n",
      " 'prediction': 'this movie did not even hard close to being boring',\n",
      " 'probability': 0.044789135}\n",
      "{'input_text': 'this movie did not even [mask] close to being boring',\n",
      " 'predicted mask token': 'not',\n",
      " 'prediction': 'this movie did not even not close to being boring',\n",
      " 'probability': 0.03614069}\n",
      "{'input_text': 'this movie did not even [mask] close to being boring',\n",
      " 'predicted mask token': 'nothing',\n",
      " 'prediction': 'this movie did not even nothing close to being boring',\n",
      " 'probability': 0.0188529}\n"
     ]
    }
   ],
   "source": [
    "def decode(tokens):\n",
    "    return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "# sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
    "sample_tokens = vectorize_layer([\"This movie did not even [mask] close to being boring.\"])\n",
    "\n",
    "# print(sample_tokens)\n",
    "prediction = bert_masked_model.predict(sample_tokens)\n",
    "# print(prediction.shape)\n",
    "\n",
    "masked_index = np.where(sample_tokens == mask_token_id)\n",
    "# print(masked_index)\n",
    "masked_index = masked_index[1]\n",
    "# print(masked_index)\n",
    "mask_prediction = prediction[0][masked_index]\n",
    "# print(mask_prediction.shape)\n",
    "print(mask_prediction[0][mask_prediction[0].argsort()[-5:][::-1]])\n",
    "top_indices = mask_prediction[0].argsort()[-5 :][::-1]\n",
    "values = mask_prediction[0][top_indices]\n",
    "\n",
    "for i in range(len(top_indices)):\n",
    "    p = top_indices[i]\n",
    "    v = values[i]\n",
    "    tokens = np.copy(sample_tokens[0])\n",
    "    tokens[masked_index[0]] = p\n",
    "    result = {\n",
    "        \"input_text\": decode(sample_tokens[0].numpy()),\n",
    "        \"prediction\": decode(tokens),\n",
    "        \"probability\": v,\n",
    "        \"predicted mask token\": id2token[p],\n",
    "    }\n",
    "    pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1ERRjisV6qH"
   },
   "source": [
    "## 감정 분류 모델 미세 조정\n",
    "\n",
    "감정 분류의 다운스트림 작업에서 자체 지도 모델을 미세 조정할 것입니다. 이를 위해 `Dense`사전 훈련된 BERT 기능 위에 풀링 계층과 계층을 추가하여 분류기를 생성해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "OQIJ-lEoV6qH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 256)]             0         \n",
      "                                                                 \n",
      " model_2 (Functional)        (None, 256, 128)          3939584   \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,947,905\n",
      "Trainable params: 8,321\n",
      "Non-trainable params: 3,939,584\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "782/782 [==============================] - 16s 19ms/step - loss: 0.7113 - accuracy: 0.5589 - val_loss: 0.6560 - val_accuracy: 0.6100\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6612 - accuracy: 0.6086 - val_loss: 0.6453 - val_accuracy: 0.6278\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6497 - accuracy: 0.6239 - val_loss: 0.6368 - val_accuracy: 0.6368\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6436 - accuracy: 0.6295 - val_loss: 0.6341 - val_accuracy: 0.6410\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6398 - accuracy: 0.6351 - val_loss: 0.6261 - val_accuracy: 0.6512\n",
      "Epoch 1/5\n",
      "782/782 [==============================] - 30s 37ms/step - loss: 0.4465 - accuracy: 0.7869 - val_loss: 0.3691 - val_accuracy: 0.8354\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.2582 - accuracy: 0.8926 - val_loss: 0.3303 - val_accuracy: 0.8618\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 29s 36ms/step - loss: 0.1331 - accuracy: 0.9501 - val_loss: 0.4077 - val_accuracy: 0.8532\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.0562 - accuracy: 0.9806 - val_loss: 0.6107 - val_accuracy: 0.8418\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.0322 - accuracy: 0.9891 - val_loss: 0.8736 - val_accuracy: 0.8072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x171f9c4b9a0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전 훈련된 bert 모델 불러오기\n",
    "mlm_model = keras.models.load_model(\n",
    "    \"bert_mlm_imdb.h5\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n",
    ")\n",
    "pretrained_bert_model = tf.keras.Model(\n",
    "    mlm_model.input, mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output\n",
    ")\n",
    "\n",
    "# 동결\n",
    "pretrained_bert_model.trainable = False\n",
    "\n",
    "\n",
    "def create_classifier_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "    sequence_output = pretrained_bert_model(inputs)\n",
    "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
    "    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "    classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    classifer_model.compile(\n",
    "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return classifer_model\n",
    "\n",
    "\n",
    "classifer_model = create_classifier_bert_model()\n",
    "classifer_model.summary()\n",
    "\n",
    "# 고정된 BERT 단계로 분류기 훈련\n",
    "classifer_model.fit(\n",
    "    train_classifier_ds,\n",
    "    epochs=5,\n",
    "    validation_data=test_classifier_ds,\n",
    ")\n",
    "\n",
    "# 미세 조정을 위해 BERT 모델 고정 해제\n",
    "pretrained_bert_model.trainable = True\n",
    "optimizer = keras.optimizers.Adam()\n",
    "classifer_model.compile(\n",
    "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "classifer_model.fit(\n",
    "    train_classifier_ds,\n",
    "    epochs=5,\n",
    "    validation_data=test_classifier_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TF1rx2-QV6qH"
   },
   "source": [
    "## 종단 간 모델 생성 및 평가\n",
    "\n",
    "모델을 배포하려는 경우 프로덕션 환경에서 사전 처리 논리를 다시 구현할 필요가 없도록 사전 처리 파이프라인이 이미 포함되어 있는 것이 가장 좋습니다. `TextVectorization`레이어 를 통합하는 종단 간 모델을 만들고 평가해 보겠습니다. 우리 모델은 원시 문자열을 입력으로 받아들입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "ATUq44ejV6qH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 11s 13ms/step - loss: 0.8736 - accuracy: 0.8072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8735891580581665, 0.807200014591217]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_end_to_end(model):\n",
    "    inputs_string = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    indices = vectorize_layer(inputs_string)\n",
    "    outputs = model(indices)\n",
    "    end_to_end_model = keras.Model(inputs_string, outputs, name=\"end_to_end_model\")\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    end_to_end_model.compile(\n",
    "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return end_to_end_model\n",
    "\n",
    "\n",
    "end_to_end_classification_model = get_end_to_end(classifer_model)\n",
    "end_to_end_classification_model.evaluate(test_raw_classifier_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlm_and_finetune_with_bert",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
